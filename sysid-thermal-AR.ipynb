{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2f5cd4",
   "metadata": {},
   "source": [
    "# Internship Finn Sherry @ Sioux Mathware\n",
    "\n",
    "---\n",
    "\n",
    "# Bayesian grey-box system identification for thermal effects\n",
    "\n",
    "Last update: 20-07-2022\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{\\boldsymbol{\\mathrm{#1}}}$\n",
    "$\\newcommand{\\covec}[1]{\\hat{\\vec{#1}}}$\n",
    "$\\newcommand{\\mat}[1]{\\boldsymbol{\\mathrm{#1}}}$\n",
    "$\\newcommand{\\inv}[1]{#1^{-1}}$\n",
    "$\\newcommand{\\given}{\\, \\vert \\,}$\n",
    "$\\newcommand{\\haslaw}{\\sim}$\n",
    "$\\newcommand{\\problaw}[1]{p(#1)}$\n",
    "$\\newcommand{\\Expectation}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Variance}{\\mathbb{V}}$\n",
    "$\\newcommand{\\Geometric}{\\textrm{Geom}}$\n",
    "$\\newcommand{\\NegBin}{\\textrm{NB}}$\n",
    "$\\newcommand{\\Poisson}{\\textrm{Pois}}$\n",
    "$\\newcommand{\\Bernoulli}{\\textrm{Bern}}$\n",
    "$\\newcommand{\\Uniform}{\\textrm{Uni}}$\n",
    "$\\newcommand{\\NormDist}{\\mathcal{N}}$\n",
    "$\\newcommand{\\GammaDist}{\\textrm{Gamma}}$\n",
    "$\\newcommand{\\ExpDist}{\\textrm{Exp}}$\n",
    "$\\newcommand{\\Uniform}{\\textrm{Uniform}}$\n",
    "$\\newcommand{\\Binomial}{\\textrm{Binom}}$\n",
    "$\\newcommand{\\BetaDist}{\\textrm{Beta}}$\n",
    "$\\newcommand{\\BetaFunc}{\\textrm{B}}$\n",
    "$\\newcommand{\\setify}[1]{\\mathbb{#1}}$\n",
    "$\\newcommand{\\NatSet}{\\setify{N}}$\n",
    "$\\newcommand{\\IntSet}{\\setify{Z}}$\n",
    "$\\newcommand{\\RealSet}{\\setify{R}}$\n",
    "$\\newcommand{\\CompSet}{\\setify{C}}$\n",
    "$\\newcommand{\\QuatSet}{\\setify{H}}$\n",
    "$\\newcommand{\\FieldSet}{\\setify{K}}$\n",
    "$\\newcommand{\\define}{:=}$\n",
    "$\\newcommand{\\enifed}{=:}$\n",
    "$\\newcommand{\\loss}{\\ell}$\n",
    "$\\newcommand{\\risk}{\\textrm{R}}$\n",
    "$\\newcommand{\\MSE}{\\textrm{MSE}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert #2 \\rVert}$\n",
    "$\\newcommand{\\InnerProduct}[2]{\\left( #1 , #2 \\right)}$\n",
    "$\\newcommand{\\kilogram}{\\textrm{kg}}$\n",
    "$\\newcommand{\\metre}{\\textrm{m}}$\n",
    "$\\newcommand{\\watt}{\\textrm{W}}$\n",
    "$\\newcommand{\\joule}{\\textrm{J}}$\n",
    "$\\newcommand{\\kelvin}{\\textrm{K}}$\n",
    "$\\newcommand{\\second}{\\textrm{s}}$\n",
    "$\\newcommand{\\centi}{\\textrm{c}}$\n",
    "$\\newcommand{\\bigO}{\\mathcal{O}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb3b5f-8b20-4396-aa58-095099a95dea",
   "metadata": {},
   "source": [
    "The primary goal of this project is to apply Bayesian techniques in order to identify parameters in a grey-box model of a thermal setup by observing the evolution in time of temperatures at a limited number of locations in the setup. Such a Bayesian approach could simultaneously estimate the relevant parameters based on easily gathered calibration data, obviating the need for difficult, expensive, or even impossible experiments to directly determine the quantities of interest from the setup. \n",
    "\n",
    "In this notebook, we will first discuss some rudimentary aspects of Bayesian learning. Subsequently, we will describe the setup, and derive a simple but reasonable grey-box model. We will also look at a number of possible model extensions, which might make it more accurate. Due to the complexity of the problem, it will turn out not to be feasible to naively apply Bayesian techniques to identify the parameters; we will have to make use of an approximate method. We will therefore experiment with two (families of) approximate techniques, Variational Bayes and Markov Chain Monte Carlo, to see which is best suited to our needs. Having chosen an approach, we will perform various experiments. For this, we will use our models to generate test data, and subsequently investigate the quality of parameter estimates. We finally conclude by summarising our results, and discussing potentially interesting open questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc171cc0-9df4-414b-92f2-286b0afec5dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayesics\n",
    "The goal of grey-box system identification is to get parameter estimates to allow for accurate state prediction. We will make use of Bayesian techniques to identify these parameters. What makes these techniques Bayesian is their reliance on Bayes' Rule:\n",
    "$$\\underbrace{p(\\vec{x} \\mid \\vec{y})}_{\\textrm{posterior}} = \\frac{\\overbrace{p(\\vec{y} \\mid \\vec{x})}^{\\textrm{likelihood}} \\overbrace{p(\\vec{x})}^{\\textrm{prior}}}{\\underbrace{p(\\vec{y})}_{\\textrm{evidence}}}.$$\n",
    "Bayes' Rule relates four different distributions:\n",
    "- _Prior_: this encodes our prior knowledge of the parameter $\\vec{x}$ before we observe $\\vec{y}$;\n",
    "- _Likelihood_: this says how the measurement $\\vec{y}$ depends on the parameter $\\vec{x}$;\n",
    "- _Evidence_: this represents the prior probability of observing $\\vec{y}$. Note that, by the Law of Total Probability, \n",
    "$$p(\\vec{y}) = \\int p(\\vec{y} \\mid \\vec{x}) p(\\vec{x}) d\\vec{x},$$\n",
    "so that the right hand side (rhs) of Bayes' Rule is properly normalised;\n",
    "- _Posterior_: this encodes our posterior knowledge of the parameter $\\vec{x}$ after we observe $\\vec{y}$.\n",
    "\n",
    "Using a Bayesian approach has two main benefits compared to e.g. [Maximum Likelihood Estimation (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation):\n",
    "- The prior allows you to incorporate your knowledge of the parameter that is being estimated;\n",
    "- You get more than just a point estimate: for instance, the width of the distribution gives you an idea of how confident you can be about your point estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e01ad-cc42-4937-b84e-63ca32378708",
   "metadata": {
    "tags": []
   },
   "source": [
    "We first became acquainted with Bayesian inference by considering the problem of [learning the bias of a coin](Bayesics_Coin.ipynb), which is the quintessential introductory application of Bayesian learning. We expanded on this by tackling the Multi-Armed Bandit (MAB) problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191e0c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## System description\n",
    "A schematic depiction of the setup is shown in the figure below. We must develop reasonable models for it, with which we can perform grey-box system identification. For a full derivation of the dynamics of the system, I refer to Chapter 2 of my report. \n",
    "<center><img src='setup_schema.png'/></center>\n",
    "In short, the setup consists of 3 metal blocks which have been lined up, with resistive nylon pads interposed. The temperature can be measured using thermistors at arbitrary places on the setup; for simplicity we assume that we measure the temperature at a single spot on each block, which we call $T_1$, $T_2$, and $T_3$. The temperatures will evolve due to a number of different factors; we will only consider the influence of conduction, convection, radiation, and the user controlled input heat. \n",
    "\n",
    "By assuming that conduction within blocks is so fast that there are no temperature differences within a block, we may model the system using a [lumped-element model](https://en.wikipedia.org/wiki/Lumped-element_model), governed by the following system of ODEs:\n",
    "$$\\frac{d}{dt}\\begin{pmatrix} m_1 c_{p, 1} T_1 \\\\ m_2 c_{p, 2} T_2 \\\\ m_3 c_{p, 3} T_3 \\end{pmatrix} = \n",
    "\\underbrace{\\begin{pmatrix} -k_{12} & k_{12} & 0 \\\\ k_{12} & -(k_{12} + k_{23}) & k_{23} \\\\ 0 & k_{23} & -k_{23} \\end{pmatrix} \\begin{pmatrix} T_1 \\\\ T_2 \\\\ T_3 \\end{pmatrix}}_{\\textrm{conduction}} + \\underbrace{\\begin{pmatrix} h(T_1, T_a, 1, t) \\\\ h(T_2, T_a, 2, t) \\\\ h(T_1, T_a, 1, t) \\end{pmatrix}}_{\\textrm{convection}} + \\underbrace{\\sigma \\begin{pmatrix} A_1 \\varepsilon_1 (T_a^4 - T_1^4) \\\\ A_2 \\varepsilon_2 (T_a^4 - T_2^4) \\\\ A_3 \\varepsilon_3 (T_a^4 - T_3^4) \\end{pmatrix}}_{\\textrm{radiation}} + \\underbrace{\\begin{pmatrix} \\Phi_1 \\\\ \\Phi_2 \\\\ \\Phi_3 \\end{pmatrix}}_{\\textrm{input}}.$$\n",
    "Convection is notoriously hard to model. Essentially the gold standard is Newton's law of cooling Clercx (2015), Eq. (8.17), which says that the convection is linear in the difference between the temperature of the block and the ambient temperature, i.e. \n",
    "$$h(T_i, T_a, i, t) = h_a (T_a - T_i),$$\n",
    "for some constant $h_a$. Furthermore, the role of radiation will often be negligible, so we could leave it out. With these simplification, our governing equations become\n",
    "$$\\frac{d}{dt}\\begin{pmatrix} m_1 c_{p, 1} T_1 \\\\ m_2 c_{p, 2} T_2 \\\\ m_3 c_{p, 3} T_3 \\end{pmatrix} = \n",
    "\\begin{pmatrix} -k_{12} & k_{12} & 0 \\\\ k_{12} & -(k_{12} + k_{23}) & k_{23} \\\\ 0 & k_{23} & -k_{23} \\end{pmatrix} \\begin{pmatrix} T_1 \\\\ T_2 \\\\ T_3 \\end{pmatrix} + h_a \\begin{pmatrix} A_1 (T_a - T_1) \\\\ A_2 (T_a - T_2) \\\\ A_3 (T_a - T_3) \\end{pmatrix} + \\begin{pmatrix} \\Phi_1 \\\\ \\Phi_2 \\\\ \\Phi_3 \\end{pmatrix},$$\n",
    "or, more compactly, \n",
    "$$ \\mat{M} \\dot{\\vec{T}} = \\mat{K} \\vec{T} + \\vec{\\Phi} + h_a \\mat{A} (\\vec{T}_a - \\vec{T}).$$\n",
    "In these equations, we can distinguish three types of quantities:\n",
    "1. Measured/observed quantities: e.g. $T_i$, $\\Phi_i$. These may vary over time, and are known up to a given accuracy due to measurement noise;\n",
    "\n",
    "2. Known constants: e.g. $m_i$, $c_{p, i}$, $T_a$. These are fully known, and are constant over time. This is reasonable for quantities such as mass (which can be easily measured) and specific heat capacity (which is a material property which according to the Dulong-Petit Law is roughly constant for metals over a long range of temperatures Carter (2000), Ch. 16) Maybe it is less reasonable for the ambient temperature (due to e.g. the setup heating up its surroundings);\n",
    "\n",
    "3. Unknown constant: e.g. $k_{ij}$, $h_a$. These are not known a priori, for instance because there is no simple physical way to measure or derive their values. For example, the conduction coefficients $k_{ij}$ can vary depending on how tightly the blocks have been put together. In this project, we want to identify these constants using Bayesian inferencing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa9962-d8e2-40dc-b845-cc786929aad3",
   "metadata": {},
   "source": [
    "#### Magnitude Analysis\n",
    "Unfortunately, the physical setup was not completed in time for us to experiment with \"real\" data. Instead, we will have to make do with artificial data, i.e. data that we have generated ourselves. So that we may experiment with parameters that are somewhat realistic, we will now estimate their typical sizes. Furthermore, we will estimate the magnitude and typical time scales of the heat exchange mechanisms. Such time scale are good to know, as they can help us design experiments that generate data on which inference is more effective. A summary of the typical magnitudes of our parameters can be found in the table below (it seems the large number of units messed up the formatting of the tableðŸ™ƒ); for the derivation I refer to Section 2.2 of my report:\n",
    "\n",
    "| Parameter | Description | Magnitude |\n",
    "| :-: | :-: | :-: |\n",
    "| $A$ | Surface area | $10^{-2}~\\metre^2$ |\n",
    "| $V$ | Volume | $10^{-3}~\\metre^3$ |\n",
    "| $m c_p$ | Heat capacity | $10^3~\\joule~\\kelvin^{-1}$ |\n",
    "| $k$ | Conduction coefficient | $10^0~\\watt~\\kelvin^{-1}$ |\n",
    "| $\\tau_{\\textrm{cond}}$ | Conduction time scale | $10^3~\\second$ |\n",
    "| $h_a$ | Convection coefficient | $10^1~\\watt~\\metre^{-2}~\\kelvin^{-1}$ |\n",
    "| $\\tau_{\\textrm{conv}}$ | Convection time scale | $10^4~\\second$ |\n",
    "| $B$ | Input amplitude | $10^1~\\watt$ |\n",
    "| $\\omega$ | Input angular frequency | $10^{-3}~\\second^{-1}$ |\n",
    "| $\\sigma$ | Measurement standard deviation | $10^{-2}~\\kelvin$ |\n",
    "\n",
    "It is important to choose a good time scale on which to observe as well as an appropriate number of observations:\n",
    "- The typical time scale of conduction is $\\tau_{\\text{cond}} = \\bigO(10^3~\\second)$, while the typical time scale of convection is $\\tau_{\\text{conv}} = \\bigO(10^4~\\second)$. Intuitively, we can only learn about the value of parameters when these parameters actually influence the observations. For instance, if we start with all temperatures being equal, we will not be able to learn anything about the conduction coefficients, because conduction will not play an important role in the evolution of the temperatures. Therefore, it seems like it would be optimal to observe over the typical time scale over which parameters are relevant. That makes this problem quite interesting, since we are trying to identify parameters whose influence plays out on time scales that differ by an order of magnitude. Consequently, we cannot choose an observation horizon that is optimal for all the parameters simultaneously. On the other hand, if all parameters were relevant on similar time scales, we might encounter unidentifiability, i.e. that the various origins of the influences on the observations cannot be distinguished.\n",
    "- In general, increasing the number of observations should improve the quality of the parameter estimates. However, it will also increase computational complexity, which could be undesirable in practice. Moreover, it would be unrealistic to assume that we can poll the temperature sensors at an arbitrary rate. \n",
    "\n",
    "After a bit of trial-and-error, we have decided to make 100 observations ($+1$ \"observation\" of the initial condition), with an observation horizon of $500~\\second$. Hence, we perform a measurement every $5~\\second$. A more structured way of deciding on the measurement setup would be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb41151",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring Approximate Techniques\n",
    "For simple problems, such as [identifying the bias of a coin](Bayesics_Coin.ipynb), we can apply Bayesian techniques analytically, producing simple equations that allow us to determine the posterior. Unfortunately, we cannot find closed form descriptions of how to update our beliefs for this problem, since the model is too complicated. Moreover, as is often the case, it is computationally intractable to apply Bayes' rule numerically SÃ¤rkkÃ¤ (2013), Sec. 1.4: Bayes' rule  tells us that\n",
    "$$\\problaw{\\vec{x} \\given \\vec{y}} = \\frac{\\problaw{\\vec{y} \\given \\vec{x}} \\problaw{\\vec{x}}}{\\problaw{\\vec{y}}}.$$\n",
    "We know the likelihood $\\problaw{\\vec{y} \\given \\vec{x}}$ and the prior $\\problaw{\\vec{x}}$. To get the posterior $\\problaw{\\vec{x} \\given \\vec{y}}$ directly from Bayes' rule we would have to determine the evidence $\\problaw{\\vec{y}}$, which involves calculating\n",
    "$$\\problaw{\\vec{y}} = \\int \\problaw{\\vec{y} \\given \\vec{x}} \\problaw{\\vec{x}} d\\vec{x}.$$\n",
    "In general, computing this integral is problematic. One issue is that the likelihood $\\problaw{\\vec{y} \\given \\vec{x}}$ is often expensive to calculate: in our case it would involve solving, at every evaluation, the system of ODEs which describe our grey-box model. Furthermore, it is typically a very high dimensional integral, and numerical methods like quadrature will consequently perform poorly Betancourt (2017). \n",
    "\n",
    "Since we cannot rely on fully analytical approaches, we have to resort to approximate techniques. We consider three different methods: Unscented Kalman filters, Variational Bayes, and Markov Chain Monte Carlo, which each have their pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b989e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Unscented Kalman filter\n",
    "\n",
    "First, we will try to apply an unscented Kalman filter (UKF), for which we use ForneyLab.jl [Cox (2019)](https://github.com/biaslab/ForneyLab.jl), a package for Julia developed by [BIASlab](https://biaslab.github.io/). For a background on the UKF, I refer to SÃ¤rkkÃ¤ (2013), Ch. 5. The experiments we ran to test the viability can be found in [this Julia notebook](Explore_UKF.ipynb).\n",
    "\n",
    "The primary issue with using a UKF from ForneyLab is that it is not yet compatible with vector based models. Hence, we would either need to implement stuff ourselves, or find some other package which already includes it. Due to the limited time we have for this project, we will not use a UKF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0d0a9",
   "metadata": {},
   "source": [
    "### Variational Bayes\n",
    "\n",
    "We will next try to apply VB. Essentially, VB works by approximating complicated distributions with ones that are easier to deal with. For instance, we might pretend some variables are independent, so that the probability density function factors. We take an entire family of such simple distributions, and then try to find the one that is closest to our true distribution in an iterative manner. It is possible to break the computations down into simple update steps, which are performed based on a factor graph which describes the relations between our variables: this makes VB suitable for online estimation on limited hardware. However, this does mean that VB is not one generic method: we need to implement corresponding \"nodes\" for the update steps. \n",
    "\n",
    "Since the corresponding nodes are not implemented in ForneyLab.jl (or in ReactiveMP.jl [Bagaev (2021)](https://github.com/biaslab/ReactiveMP.jl), another package by BIASlab), we have to look around elsewhere. [Luttinen (2013)](https://link.springer.com/chapter/10.1007/978-3-642-40988-2_20) wrote a paper about using VB to identify the state update matrix in a Linear State-Space Model (LSSM). For this, he developed the Python package [BayesPy](https://github.com/bayespy). In fact, it contains a bunch of tricks in order to speed up inference, which is not so relevant to our problem. \n",
    "\n",
    "While Julia can call Python (and a whole bunch of other popular languages), we have applied it in a separate [Python Jupyter notebook](Explore_VB.ipynb).\n",
    "\n",
    "There are numerous issues with VB that make it unsuitable for application in this project, which could be mostly solved by investing a sufficient amount of time:\n",
    "- Even without real process noise, the inference is very sensitive to the prior of the innovation $\\nu$;\n",
    "- The ARD as is currently applied in BayesPy does not make a lot of sense for our problem. We would have to implement a componentwise (instead of columnwise) ARD. That might help with the sensitivity to the prior of the innovation too;\n",
    "- Our problem is somewhat more complex, involving also convection and input heats. It is not clear how these can be added using existing software;\n",
    "- It is not clear how to convert knowledge of the distribution of components in matrices into posteriors for the underlying parameters.\n",
    "\n",
    "Consequently I will not make use of VB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64736965",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Markov Chain Monte Carlo\n",
    "The final approximate method we will consider is Markov Chain Monte Carlo (MCMC). Using MCMC we approximate our posterior by first constructing a Markov chain. It has to be ergodic, so that a stationary distribution exists and can be interpreted as an equilibrium distribution. The Markov chain we constuct should have the desired posterior as stationary distribution. This can be done using a Metropolis-Hastings (MH) algorithm, such as Hamiltonian Monte Carlo (HMC). For a brief background on MCMC, MH, and HMC, I refer to Section 3.2 of my report. [Betancourt (2017)](http://arxiv.org/abs/1701.02434) provides some deeper intuition about HMC.\n",
    "\n",
    "Since we are constructing a Markov chain, MCMC generates new samples iteratively:\n",
    "- We start with the current sample $\\vec{z}_n$;\n",
    "- We somehow stochastically perturb $\\vec{z}_n$ to $\\hat{\\vec{z}}_{n + 1}$. The most basic method is to use a random walk: this is the original Metropolis algorithm [Metropolis (1953)](https://doi.org/10.1063/1.1699114). We can associate a distribution $g(\\hat{\\vec{z}}_{n + 1} \\given \\vec{z}_n)$ with this. \n",
    "- We calculate the following acceptance probability:\n",
    "$$\\rho(\\vec{z}_n, \\hat{\\vec{z}}_{n + 1}) = \\frac{\\problaw{\\hat{\\vec{z}}_{n + 1} \\given \\vec{y}}}{\\problaw{\\vec{z}_n \\given \\vec{y}}}\\frac{g(\\vec{z}_n \\given \\hat{\\vec{z}}_{n + 1})}{g(\\hat{\\vec{z}}_{n + 1} \\given \\vec{z}_n)};$$\n",
    "- If we accept, then we continue with $\\vec{z}_{n + 1} = \\hat{\\vec{z}}_{n + 1}$, else we reuse $\\vec{z}_{n + 1} = \\vec{z}_n$.\n",
    "\n",
    "This indeed produces a Markov chain, which under some assumptions of regularity has the desired equilibrium properties (for more details, see e.g. [Robert (2004)](http://link.springer.com/10.1007/978-1-4757-4145-2), Ch. 7). \n",
    "We can initialise our Markov chain in multiple different places if we want to check ergodicity. Notably, we only need a function that is proportional to the posterior to construct our Markov chain: Bayes' rule tells us that the product of the likelihood and the prior is such a function. \n",
    "\n",
    "When we start of with sampling, we will in general not yet be in equilibrium. We therefore first sample a bunch of times to reach equilibrium. This is called the _burn-in period_, and these samples are discarded. We then continue to sample: since we are in equilibrium, these samples will come from the stationary distribution, and by construction therefore approximate the posterior. \n",
    "\n",
    "The exploratory application of MCMC to our problem can be found in [this Julia Jupyter notebook](Explore_MCMC.ipynb). From these experiments, it seems like MCMC, implemented by combining Turing.jl [Ge (2018)](https://github.com/TuringLang/Turing.jl) with DifferentialEquations.jl [Rackaukas (2017)](https://github.com/SciML/DifferentialEquations.jl), is a good approximate technique for performing Bayesian inference:\n",
    "- The method is fairly general, which means that it should not be necessary to develop (parts of) software packages;\n",
    "- The method seems to be fairly insensitive to how informative the priors are, although this might change if we were to introduce some process noise;\n",
    "- The accuracy (in terms of MSE) of the inference seems to scale with the measurement noise in a roughly linear way.\n",
    "\n",
    "MCMC is fairly slow however, and so it would not be suitable for online parameter estimation. It would be more suited to occasional calibrations. This is not a problem for my project, however. Hence, during the rest of this project we will work with MCMC. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab67f8-33f1-4707-85f7-13108a258b32",
   "metadata": {},
   "source": [
    "## Results\n",
    "We will experiment by expanding our physical and probabilistic models, and testing combinations of different models for generation and identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5072bc-50b6-4328-b312-395b658f69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\");\n",
    "Pkg.instantiate();\n",
    "IJulia.clear_output();\n",
    "using Turing, DifferentialEquations, Random, LinearAlgebra, StatsBase # Computational\n",
    "using Measures, LaTeXStrings, StatsPlots # Formatting\n",
    "default(label=\"\");\n",
    "Random.seed!(987654321); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa10bbc-3756-423d-81ff-9a8cf9596f8a",
   "metadata": {},
   "source": [
    "### Lumped-Element Model\n",
    "#### Physical Model\n",
    "We will start with our most basic lumped-element model: the temperature does not vary within a block, and radiation is ignored; the same model as we used in [our notebook for MCMC](Explore_MCMC.ipynb). Here, we will pay more attention to getting realistic system parameter values. First, we need to load all the necessary packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b701c6-45a3-4bdf-8316-ad3533c03132",
   "metadata": {},
   "source": [
    "With the following functions, we can then define the input heats and the system of ODEs that govern the basic lumped-element model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfe4fc-f9fe-4162-ad26-f0a095befa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input heat\n",
    "Î¦(B, Ï‰, t) = B * sin(Ï‰ * t) # Julia allows you to define simple functions by assignment\n",
    "\n",
    "# System of ODEs governing our basic lumped-element model\n",
    "function LSSM_lump(dT, T, p, t)\n",
    "    mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, k12, k23, h_a = p\n",
    "    # Conduction\n",
    "    dT[1] = k12 * (T[2] - T[1]) / mcp_1\n",
    "    dT[2] = (k12 * (T[1] - T[2]) + k23 * (T[3] - T[2])) / mcp_2\n",
    "    dT[3] = k23 * (T[2] - T[3]) / mcp_3\n",
    "    # Convection\n",
    "    dT[1] += h_a * A_1 * (T_a - T[1]) / mcp_1\n",
    "    dT[2] += h_a * A_2 * (T_a - T[2]) / mcp_2\n",
    "    dT[3] += h_a * A_3 * (T_a - T[3]) / mcp_3\n",
    "    # Input\n",
    "    dT[1] += Î¦(B_1, Ï‰_1, t) / mcp_1\n",
    "    dT[2] += Î¦(B_2, Ï‰_2, t) / mcp_2\n",
    "    dT[3] += Î¦(B_3, Ï‰_3, t) / mcp_3\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20adbd-1f75-4553-b10f-98e464d332a7",
   "metadata": {},
   "source": [
    "#### Simulate Data\n",
    "Next, we will simulate data to perform inference on. First, we will define the constants. Of these, only $k_{12}$, $k_{23}$, and $h_a$ are unknown: the ambient temperature $T_a$ and the initial temperatures $\\vec{T}_0$ are assumed to be fully known (although it would not be very difficult to treat them as unknowns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83380fdb-f4c2-4fce-a8f5-f446e912168c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Time horizon\n",
    "sample_size = 100\n",
    "Î” = 5 # Final time t = 5 x 10^2 s\n",
    "time = [Î” * i for i in 0:sample_size] \n",
    "# Constants of blocks\n",
    "true_mcp_1 = 1e3\n",
    "true_mcp_2 = 1.5e3\n",
    "true_mcp_3 = 0.8e3\n",
    "true_A_1 = 1e-2\n",
    "true_A_2 = 1.5e-2\n",
    "true_A_3 = 2e-2\n",
    "# Input heat parameters\n",
    "true_B_1 = 3e1\n",
    "true_B_2 = -3e1\n",
    "true_B_3 = 1.5e1\n",
    "true_Ï‰_1 = 3e-2\n",
    "true_Ï‰_2 = 2.4e-2\n",
    "true_Ï‰_3 = 3.8e-2\n",
    "# Temperatures\n",
    "true_T_a = 290.\n",
    "T_0 = [330., 270., 310.]\n",
    "# Unknown constants\n",
    "true_k12 = 3e0\n",
    "true_k23 = 2e0\n",
    "true_h_a = 1e1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ec5bf-3390-4453-8650-78ccbadcbc8f",
   "metadata": {},
   "source": [
    "With these parameters, we can now generate our observations. The measurement noise is quite small, with a standard deviation of roughly $10^{-2}~\\kelvin$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77142021-7760-48f5-98a4-e3d3f5dbcab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot observations over true evolution\n",
    "function observ_plot(solution, data, horizon)\n",
    "    p_obs = plot(horizon, solution; legend = true, xlim = (horizon[1], horizon[end]), ylim = (260, 340), linecolors = [\"red\" \"blue\" \"orange\"], labels = [L\"$T_1$ true\" L\"$T_2$ true\" L\"$T_3$ true\"], xlabel = L\"t~(\\textrm{s})\", ylabel = L\"T~(\\textrm{K})\", size = (1200, 400), bottommargin = 6mm, leftmargin = 6mm)\n",
    "    scatter!(p_obs, time, data', markercolors = [\"red\" \"blue\" \"orange\"], labels = [L\"$T_1$ observed\" L\"$T_2$ observed\" L\"$T_3$ observed\"])\n",
    "    return p_obs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fabfe7-e07e-44a7-ab79-be4f1b176726",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, true_T_a, true_k12, true_k23, true_h_a] # First known, then unknown parameters\n",
    "# Solve the system numerically using DifferentialEquations.jl\n",
    "LSSM_dynamics_lump = ODEProblem(LSSM_lump, T_0, (time[1], time[end]), true_p)\n",
    "true_sol = solve(LSSM_dynamics_lump, Tsit5(); saveat = Î”, verbose = false)\n",
    "true_Ïƒ = 1e-2\n",
    "y = Array(true_sol) + true_Ïƒ * randn(size(Array(true_sol)))\n",
    "plot_obs = observ_plot(Array(true_sol)', y, time)\n",
    "# savefig(plot_obs, \"Results\\\\Expand\\\\lump_obs.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e5055-2017-4c06-8d47-db44ffc1dee4",
   "metadata": {},
   "source": [
    "This seems like an ideal problem for a heat map. We can imagine the 1D thermal set up in the vertical direction, which evolves over time on the horizontal axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894bfd8-4024-4d50-a082-9529f670ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 5 labels on time axis\n",
    "function heatmap_time(horizon, time_step)\n",
    "    steps = 5\n",
    "    N = length(horizon)\n",
    "    axis = Vector{String}(undef, N)\n",
    "    axis .= \"\" \n",
    "    step = div(N, steps)\n",
    "    for i âˆˆ 0:steps\n",
    "        axis[i * step + 1] = string(i * step * time_step)\n",
    "    end\n",
    "    return axis\n",
    "end\n",
    "\n",
    "# If there are few slices, show label for each slice.\n",
    "function heatmap_few_temps(slice_count)\n",
    "    axis = string.(1:slice_count)\n",
    "    return axis\n",
    "end\n",
    "\n",
    "# If there are many slices, only show labels for 9 slices\n",
    "function heatmap_many_temps(slice_count)\n",
    "    steps = 9\n",
    "    axis = Vector{String}(undef, slice_count)\n",
    "    axis .= \"\" \n",
    "    step = div(slice_count, 2 * steps)\n",
    "    for i âˆˆ 1:steps\n",
    "        axis[(2 * i - 1) * step] = string((2 * i - 1) * step)\n",
    "    end\n",
    "    return axis\n",
    "end\n",
    "\n",
    "# Create heatmap of true temperature evolution\n",
    "function state_heatmap(solution, horizon, time_step)\n",
    "    time_axis = heatmap_time(horizon, time_step)\n",
    "    time_grid_end = length(time_axis)\n",
    "    slice_count = size(solution)[1]\n",
    "    if slice_count < 24\n",
    "        block_axis = heatmap_few_temps(slice_count)\n",
    "    else\n",
    "        block_axis = heatmap_many_temps(slice_count)\n",
    "    end\n",
    "    block_grid_end = length(block_axis)\n",
    "    hmap = heatmap(solution, xticks = (1:time_grid_end, time_axis), yticks = (1:block_grid_end, block_axis), xlabel = L\"t~(\\textrm{s})\", ylabel = \"Slice\")\n",
    "    return hmap\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c79c4b-6d45-43fe-8e2f-385f4ef1aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmap = state_heatmap(Array(true_sol), time, Î”)\n",
    "# savefig(hmap, \"Results\\\\Expand\\\\lump_obs_heatmap.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada48315-7a3f-49b3-acb6-291d38f892d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Probabilistic Model\n",
    "Next, we define the probabilistic model, i.e. the model that is used for the identification. The parameters we wish to identify are $h_a$, $k_{12}$, and $k_{23}$. It would not make sense for any of these to be negative, since this would lead to heat flowing from cold sources into hot sinks, in contravention of the [Second Law of Thermodynamics](https://en.wikipedia.org/wiki/Laws_of_thermodynamics#Second_law) Carter (2000), Ch. 6. Hence, we make use of Gamma priors. Finally, we treat the standard deviation of the measurement noise as an unknown quantity. This might help by absorbing any other sources of (process) noise. It would again be sensible to use a positively supported prior like the Gamma distribution for this. We pose reasonably broad priors (this does not seem to be very important in this case: it seems like there is a lot of learning going on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7112d-9086-4315-8142-2d8ee64b69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Turing our probabilistic model is based on the lumped-element model, with normally distributed noise\n",
    "@model function fit_LSSM_lump(data, system, syst_consts)\n",
    "    # Gamma in Distributions.jl is shape-scale, not shape-rate\n",
    "    Ïƒ ~ Gamma(1e-2, 1e0) # E[Ïƒ] = 10^-2, Var(Ïƒ) = 10^-2 \n",
    "    k12 ~ Gamma(1e0, 1e0) # E[k] = 10^0, Var(k) = 10^0\n",
    "    k23 ~ Gamma(1e0, 1e0) \n",
    "    h_a ~ Gamma(1e1, 1e0) # E[h_a] = 10^1, Var(h_a) = 10^1\n",
    "    T_a, mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, Î” = syst_consts\n",
    "    p = [mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, k12, k23, h_a]\n",
    "    predicted = solve(system, Tsit5(); p = p, saveat = Î”, verbose = false)\n",
    "\n",
    "    for i âˆˆ 1:length(predicted)\n",
    "        data[:, i] ~ MvNormal(predicted[i], Ïƒ^2 * I)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89160d0d-35e7-43d5-af40-70afd643c0fa",
   "metadata": {},
   "source": [
    "With Turing.jl and DifferentialEquations.jl, defining the probabilistic model is very easy. First, we pose the priors. With MCMC, we do not have to worry about conjugate priors. In general, as long as the support of the prior is infinite, semi-infinite, or an interval, as is the case for most nice continuous random variables, Turing.jl is able to work (under the hood, it automatically performs a transformation to make the support infinite, which is required for HMC (derived) methods). We subsequently take in all of the known parameters. Next, we determine how, given a certain sample of unknown parameters, the temperatures would evolve. Finally, we give the corresponding distribution of each observation, in this case a normal distribution with mean equal to the temperature predicted based on the current sample and fixed variance $\\sigma^2$. The likelihood (for that sample of unknown parameters) will then be the product of all these distributions, i.e. the joint distribution (since the noise of each measurement is independent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5d82c-3e4f-46b2-9b8f-020fcfc25d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inference\n",
    "Finally, we can perform the sampling. We make use of the [No U-Turn Sampling (NUTS) sampler](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo#No_U-Turn_Sampler) in [Turing.jl](https://github.com/TuringLang/Turing.jl), which can be seen as building on Hamiltonian Monte Carlo (HMC). One advantage of this sampler is that we only have to choose the acceptance rate: it will automatically tune a bunch of internal parameters (e.g. step size) to achieve this acceptance rate. A popular acceptance rate is 0.65, since Beskos et al. in their 2010 paper [\"Optimal Tuning of the Hybrid Monte-Carlo Algorithm\"](https://arxiv.org/abs/1001.4460v1) showed that this is asymptotically the optimal choice in the sense of balancing the cost of proposal generation with the typical number of required proposals per sample. Finally, we take 2500 samples per chain, for which Turing automatically chooses a burn-in period of 1000 samples. To check whether we have reached equilibrium after the burn-in period, we sample three independent Markov chains: if the resulting empirical posteriors roughly correspond, we can be reasonably confident that we had reached equilibrium before taking the \"real\" samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e53f2b-8da5-4649-97a0-a667197b27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lump = fit_LSSM_lump(y, LSSM_dynamics_lump, [true_T_a, true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, Î”]);\n",
    "chain_lump = sample(model_lump, NUTS(0.65), MCMCSerial(), 2500, 3; verbose = false, progress = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0ee25-cf60-41b2-a04f-dfcd4845c688",
   "metadata": {},
   "source": [
    "In order to plot the results, we sample from the Markov chain. For each set of sampled parameters, we generate a trajectory using our system of ODEs, which we overlay on the same plot. This is an alternative to trying to compute credible intervals directly using the posteriors. We only take 300 samples instead of making use of all samples in order to reduce the size of the resulting image. Too many elements in the figure will make it slow to generate, and even slow to scroll past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d5652-e1ab-419c-ae1c-55a188719058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sampled trajectories and observations over true evolution\n",
    "function sampled_traj_plot(params_samples, solution, data, horizon, time_step, all_params, id_count, system, obs_slices = nothing)\n",
    "    p_traj = plot(; legend = true, xlim = (horizon[1], horizon[end]), ylim = (260, 340), ylabel = L\"T~(\\textrm{K})\", size = (1200, 400), bottommargin = 6mm, leftmargin = 6mm)\n",
    "    params_cur = copy(all_params)\n",
    "    for (i, params_row) âˆˆ enumerate(eachrow(params_samples))\n",
    "        params_cur[(end - id_count):end] .= params_row[(end - id_count):end]\n",
    "        traj_cur = solve(system, Tsit5(); p = params_cur, saveat = time_step)\n",
    "        if size(Array(traj_cur))[1] == 3\n",
    "            plot!(p_traj, traj_cur; alpha = 0.05, linecolors = [\"red\" \"blue\" \"orange\"], label = \"\")\n",
    "        else\n",
    "            plot!(p_traj, horizon, Array(traj_cur)[obs_slices, :]'; alpha = 0.05, linecolors = [\"red\" \"blue\" \"orange\"], label = \"\")\n",
    "        end\n",
    "    end\n",
    "    if obs_slices == nothing\n",
    "        plot!(p_traj, solution, linecolors = [\"red\" \"blue\" \"orange\"], linewidth = 1, labels = [L\"T_1\" L\"T_2\" L\"T_3\"]) \n",
    "    else \n",
    "        plot!(p_traj, horizon, Array(solution)[obs_slices, :]', linecolors = [\"red\" \"blue\" \"orange\"], linewidth = 1, labels = [L\"T_1\" L\"T_2\" L\"T_3\"]) \n",
    "    end\n",
    "    scatter!(p_traj, horizon, data', xlabel = L\"t~(\\textrm{s})\", markercolors = [\"red\" \"blue\" \"orange\"])\n",
    "    return p_traj\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa98cb-1ac6-4bcb-b094-b8da32363f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior_samples = Array(sample(chain_lump, 300; replace = false))\n",
    "plot_sol_app = sampled_traj_plot(posterior_samples, true_sol, y, time, Î”, true_p, 2, LSSM_dynamics_lump)\n",
    "# savefig(plot_sol_app, \"Results\\\\Expand\\\\lump_states.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33038ed6-0d42-4d09-8aac-46f2ed828d23",
   "metadata": {},
   "source": [
    "We cannot see the sampled trajectories, because they lie so close to the true trajectories. Since the dynamics resulting from the samples is not too far off from the real dynamics, it would seem like the inference has been highly successful.\n",
    "\n",
    "Let's have a look at the (approximate) posteriors of the parameters. We will quantify the quality of the estimates by calculating the MSE of the posteriors with respect to the true value. The bias-variance decomposition makes this an easy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a491b-d577-4a8a-9f6d-f93ca119ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSE of true value compared to posterior\n",
    "get_mean(Î¸::Symbol, chain) = (summarize(chain[[Î¸]]).nt[:mean][1]\n",
    "get_var(Î¸::Symbol, chain) = (summarize(chain[[Î¸]]).nt[:std])[1]^2\n",
    "MSE(Î¸::Symbol, true_Î¸, chain) = (get_mean(Î¸, chain) - true_Î¸)^2 + get_var(Î¸, chain)\n",
    "\n",
    "# Format MSE as nice string for on plot\n",
    "sci_not(value, sigdigits = 2) = replace(\"$(round(value, sigdigits = sigdigits))\", r\"e(-?\\d+)\" => s\"\\\\times 10^{\\1}\") # Regex magic\n",
    "\n",
    "function MSE_string(Î¸::Symbol, true_Î¸, chain)\n",
    "    MSE_Î¸ = MSE(Î¸, true_Î¸, chain)\n",
    "    label = latexstring(\"\\\\textrm{MSE} = \" * sci_not(MSE_Î¸))\n",
    "    return label\n",
    "end\n",
    "\n",
    "# Plot marginal posterior\n",
    "function marg_post_plot(Î¸::Symbol, true_Î¸, chain, ymax = nothing)\n",
    "    if ymax == nothing\n",
    "        p_Î¸ = density(chain[[Î¸]], title = L\"%$Î¸\", label = \"\", legend = true)      \n",
    "    else\n",
    "        p_Î¸ = density(chain[[Î¸]], title = L\"%$Î¸\", label = \"\", legend = true, ylim = (0, ymax))\n",
    "    end\n",
    "    vline!(p_Î¸, [true_Î¸], label = L\"True $%$Î¸$\")\n",
    "    MSE_label = MSE_string(Î¸, true_Î¸, chain)\n",
    "    annotate!(p_Î¸, (0.05, 0.95), (MSE_label, 12, :left))\n",
    "    return p_Î¸\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d431c15-966d-421d-9609-33152c822bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_Ïƒ = marg_post_plot(:Ïƒ, true_Ïƒ, chain_lump, 600)\n",
    "p_k12 = marg_post_plot(:k12, true_k12, chain_lump, 900)\n",
    "p_k23 = marg_post_plot(:k23, true_k23, chain_lump, 600)\n",
    "p_h_a = marg_post_plot(:h_a, true_h_a, chain_lump, 14)\n",
    "plot_marg_post = plot(p_Ïƒ, p_k12, p_k23, p_h_a, size = (1200, 800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(plot_marg_post, \"Results\\\\Expand\\\\lump_params.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2905cfc-2da6-4f7e-b48b-6f0cce3a846e",
   "metadata": {},
   "source": [
    "Indeed, the empirical posteriors of the unknown parameters look good: they peak close to the true values, and are rather narrow (width of $~10^{-1}$ for convection, $~10^{-3}$ for conduction). One should bear in mind that the observations are random due to the noise, so small deviations of the MAP from the true value do not necessarily indicate something has gone wrong. If we ran the tests again, we would see the MAPs shift slightly. \n",
    "\n",
    "On the other hand, the approximate posterior of the standard deviation of the measurement noise looks very far from the true value. This does appear to suggest a systematic problem with the inference. I believe this is due to the fact that our discretisation effectively introduces process noise. This process noise is indistinguishable from measurement noise, and simply gets added on top. However, this does not have to be a problem: we are not really interested in learning the measurement noise, and the fact that it can absorb process noise is a nice property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddeecd8-327c-4e04-92a1-0ae4c60f3a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corn = cornerplot(posterior_samples, label = [L\"Ïƒ\", L\"k_{12}\", L\"k_{23}\", L\"h_a\"], size = (1800, 1800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(corn, \"Results\\\\Expand\\\\lump_corner.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5d1d9-7619-40fb-a65f-59c416e6f836",
   "metadata": {},
   "source": [
    "#### Validation\n",
    "The previous experiment gives a promising result: it seems like the parameters identified using a more simple model can accurately reproduce the true physics. One issue, however, is that the perceived accuracy may be due to overfitting. In this section, we would like to put the parameter estimates combined with the simple lumped-element model to the test. We could do this in multiple ways:\n",
    "1) We could change some of the known parameters;\n",
    "2) We could simulate further time, i.e. we identify based on say the first $500~\\second$, and compare the predicted and true trajectories over the subsequent $500~\\second$;\n",
    "3) We could identify on a subset of the data, say the first $100~\\second$, and compare the predicted and true trajectories on the remaining data;\n",
    "4) We could change the input heats and initial temperatures.\n",
    "\n",
    "The first method does not seem practically applicable: in the true setup, these types constants cannot be changed without disassembly, which should probably be followed by more calibration, anyway. In our case, the second method does not sound like it would be very useful, either: the state of our setup naturally moves towards an equilibrium over time, and therefore, the errors would become smaller over time. This means that we would not be able properly assess how good the state prediction is. To mitigate this issue, we could make use of the third approach. Finally, changing the input heats and initial temperatures seems like something that would be relevant in practice, and so the fourth method seems reasonable. We have decided to use the latter of these two viable methods, since it would seem to more drastically change the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7783e-17fe-41bb-9d16-b0e28922931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Block input signal: works, but seems to make stuff unstable\n",
    "# function Î¦_val(B, t_start, t_end, t)\n",
    "#     if t_start < t < t_end\n",
    "#         return B\n",
    "#     else \n",
    "#         return 0\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# Smoothed block input signal, might reduce numerical instabilities\n",
    "sigmoid(x, a = 0, l = 1) = 1 / (1 + exp(-l * (x - a)))\n",
    "Î¦_val_smooth(B, t_start, t_end, t, l = 2e-1) = B * sigmoid(t, t_start, l) * sigmoid(-t, -t_end, l)\n",
    "\n",
    "# System of ODEs to validate the lumped-element model for prediction\n",
    "function LSSM_lump_val(dT, T, p, t)\n",
    "    mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, t_start_1, t_start_2, t_start_3, t_end_1, t_end_2, t_end_3, T_a, k12, k23, h_a = p\n",
    "    # Conduction\n",
    "    dT[1] = k12 * (T[2] - T[1]) / mcp_1\n",
    "    dT[2] = (k12 * (T[1] - T[2]) + k23 * (T[3] - T[2])) / mcp_2\n",
    "    dT[3] = k23 * (T[2] - T[3]) / mcp_3\n",
    "    # Convection\n",
    "    dT[1] += h_a * A_1 * (T_a - T[1]) / mcp_1\n",
    "    dT[2] += h_a * A_2 * (T_a - T[2]) / mcp_2\n",
    "    dT[3] += h_a * A_3 * (T_a - T[3]) / mcp_3\n",
    "    # Input\n",
    "    dT[1] += Î¦_val_smooth(B_1, t_start_1, t_end_1, t) / mcp_1\n",
    "    dT[2] += Î¦_val_smooth(B_2, t_start_2, t_end_2, t) / mcp_2\n",
    "    dT[3] += Î¦_val_smooth(B_3, t_start_3, t_end_3, t) / mcp_3\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c1f68-c237-47e4-a3b1-ae7c6e73ce64",
   "metadata": {},
   "source": [
    "We now generate the validation data set, which consists of the trajectories for the true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be115af8-8d3e-44ae-abab-5caf322286ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input signals used for validation\n",
    "function val_input_plot(B_1, B_2, B_3, t_start_1, t_start_2, t_start_3, t_end_1, t_end_2, t_end_3, horizon)\n",
    "    ymax = round(maximum([B_1, B_2, B_3]), sigdigits = 2, RoundUp)\n",
    "    p_input = plot(horizon, Î¦_val_smooth.(B_1, t_start_1, t_end_1, horizon), title = \"Input\", xlim = (horizon[1], horizon[end]), ylim = (0, ymax), label = L\"Î¦_1\", linecolor = \"red\", xlabel = L\"t~(\\textrm{s})\", ylabel = L\"Î¦~(\\textrm{W})\", legend = true)\n",
    "    plot!(p_input, horizon, Î¦_val_smooth.(B_2, t_start_2, t_end_2, horizon), label = L\"Î¦_2\", linecolor = \"blue\")\n",
    "    plot!(p_input, horizon, Î¦_val_smooth.(B_3, t_start_3, t_end_3, horizon), label = L\"Î¦_3\", linecolor = \"orange\")\n",
    "    return p_input\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11555f4-f9a2-464e-bdf3-9cf1e63bd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose parameters of smoothed block waves for validation input signals\n",
    "val_B_1, val_B_2, val_B_3 = rand(Gamma(2.5e1, 2e0), 3)\n",
    "val_t_start_1, val_t_start_2, val_t_start_3 = rand(Gamma(5e1, 4e0), 3)\n",
    "val_b_width_1, val_b_width_2, val_b_width_3 = rand(Gamma(2e1, 5e0), 3)\n",
    "val_t_end_1, val_t_end_2, val_t_end_3 = val_t_start_1 + val_b_width_1, val_t_start_2 + val_b_width_2, val_t_start_3 + val_b_width_3\n",
    "# Time horizon\n",
    "sample_size_val = 1000\n",
    "Î”_val = time[end] / sample_size_val\n",
    "time_val = [Î”_val * i for i in 0:sample_size_val] \n",
    "val_input_plot(val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, time_val)\n",
    "val_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, true_T_a, true_k12, true_k23, true_h_a] # First known, then unknown parameters\n",
    "LSSM_dynamics_lump_val = ODEProblem(LSSM_lump_val, T_0, (time_val[1], time_val[end]), val_p)\n",
    "val_sol = solve(LSSM_dynamics_lump_val, Tsit5(); saveat = Î”_val, verbose = false);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b4546-09a9-47c1-bb77-3bed04600304",
   "metadata": {},
   "source": [
    "We then generate the trajectories for 300 sampled sets of parameters. To see how \"good\" these sampled trajectories are, we create a number of different diagrams, including a plot of the residuals, and histograms of the MSE between the true and the sampled trajectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cab4c-afb5-464d-9c73-c87547f53ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals of sampled trajectories compared to true trajectories\n",
    "function compute_resid(traj, solution, obs_slices = nothing)\n",
    "    if size(Array(solution))[1] == 3\n",
    "        resid = Array(traj) - Array(solution)\n",
    "    elseif size(Array(traj))[1] == 3 && size(Array(solution))[1] > 3\n",
    "        resid = Array(traj) - Array(solution)[obs_slices, :]\n",
    "    else\n",
    "        resid = Array(traj)[obs_slices, :] - Array(solution)[obs_slices, :]\n",
    "    end\n",
    "    return resid\n",
    "end\n",
    "\n",
    "# Plot summaries of validation\n",
    "function sampled_traj_val_plot(params_samples, solution, horizon, time_step, all_params, Ïƒ, id_count, system, obs_slices = nothing)\n",
    "    p_traj = plot(; legend = true, xlim = (horizon[1], horizon[end]), title = \"Trajectories\", ylim = (260, 340), ylabel = L\"T~(\\textrm{K})\", bottommargin = 6mm, leftmargin = 6mm)\n",
    "    p_error = plot(; legend = true, xlim = (horizon[1], horizon[end]), title = \"Residual\", xlabel = L\"t~(\\textrm{s})\", ylabel = L\"Î”T~(\\textrm{K})\", labels = [L\"Î”T_1\" L\"Î”T_2\" L\"Î”T_3\"], bottommargin = 6mm, leftmargin = 6mm)\n",
    "    emp_mse = Matrix{Float64}(undef, 3, size(params_samples)[1])\n",
    "    params_cur = copy(all_params)\n",
    "    for (i, params_row) âˆˆ enumerate(eachrow(params_samples))\n",
    "        params_cur[(end - id_count):end] .= params_row[(end - id_count):end]\n",
    "        traj_cur = solve(system, Tsit5(); p = params_cur, saveat = time_step)\n",
    "        if size(Array(traj_cur))[1] == 3\n",
    "            plot!(p_traj, traj_cur; alpha = 0.05, linecolors = [\"red\" \"blue\" \"orange\"], label = \"\")\n",
    "        else\n",
    "            plot!(p_traj, horizon, Array(traj_cur)[obs_slices, :]'; alpha = 0.05, linecolors = [\"red\" \"blue\" \"orange\"], label = \"\")\n",
    "        end\n",
    "        resid_cur = compute_resid(traj_cur, solution, obs_slices)\n",
    "        emp_mse[:, i] = mean(abs2, resid_cur, dims = 2)\n",
    "        plot!(p_error, horizon, resid_cur'; alpha = 0.05, linecolors = [\"red\" \"blue\" \"orange\"], label = \"\")\n",
    "    end\n",
    "    hline!(p_error, [(2 * Ïƒ) -(2 * Ïƒ)], linestyles = [:dash :dash], linecolors = [\"black\" \"black\"], labels = [L\"\\pm 2 \\sigma\" \"\"])\n",
    "    if size(Array(solution))[1] == 3\n",
    "        plot!(p_traj, solution, linecolors = [\"red\" \"blue\" \"orange\"], linewidth = 1, labels = [L\"T_1\" L\"T_2\" L\"T_3\"]) \n",
    "    else \n",
    "        plot!(p_traj, horizon, Array(solution)[obs_slices, :]', linecolors = [\"red\" \"blue\" \"orange\"], linewidth = 1, xlabel = L\"t~(\\textrm{s})\", labels = [L\"T_1\" L\"T_2\" L\"T_3\"]) \n",
    "    end\n",
    "    h_mse_1 = histogram(emp_mse[1, :], normalize = :pdf, title = L\"\\textrm{MSE}~T_1\", xlabel = L\"\\textrm{MSE}~(\\textrm{K}^2)\", ylabel = \"Density\", color = \"red\")\n",
    "    h_mse_2 = histogram(emp_mse[2, :], normalize = :pdf, title = L\"\\textrm{MSE}~T_2\", xlabel = L\"\\textrm{MSE}~(\\textrm{K}^2)\", ylabel = \"Density\", color = \"blue\")\n",
    "    h_mse_3 = histogram(emp_mse[3, :], normalize = :pdf, title = L\"\\textrm{MSE}~T_3\", xlabel = L\"\\textrm{MSE}~(\\textrm{K}^2)\", ylabel = \"Density\", color = \"orange\")\n",
    "    return p_traj, p_error, h_mse_1, h_mse_2, h_mse_3\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b360f-b00c-4527-ad27-f5d9e2321880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_in = val_input_plot(val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, time_val)\n",
    "p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3 = sampled_traj_val_plot(posterior_samples, val_sol, time_val, Î”_val, val_p, true_Ïƒ, 2, LSSM_dynamics_lump_val)\n",
    "plot_val = plot(p_in, p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3, size = (1800, 800), layout = (2, 3), leftmargin = 10mm, bottommargin = 10mm)\n",
    "# savefig(plot_val, \"Results\\\\Validation\\\\lump_val.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387ea27-a5fb-450e-9c20-71769c4ea90f",
   "metadata": {},
   "source": [
    "The top left diagram shows the used input signals; the start, end, and height of the blocks have been chosen randomly. Next, we have plotted the corresponding true trajectories, with the sampled trajectories overlaid. The top right diagram shows the residuals, i.e. the difference between each sampled trajectory and the true trajectory. For small $t$, the residuals grow: this is due to the fact that the initial temperatures are treated as known constants. Eventually, the residuals tend to stabilise, with the difference remaining less than about $10^{-2}~\\kelvin$, which is good considering the typical temperatures are $10^2~\\kelvin$ and the measurement accuracy is also about $10^{-2}~\\kelvin$. The histograms on the bottom row show histograms of the \\gls{mse} for each temperature. Each data point represents the MSE over an entire sampled trajectory. If the mean error were normally distributed, which does not sound unreasonable, then we would expect the MSE to follow a (scaled) $\\chi^2$-distribution. This indeed appears to be the case looking at the histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b0a5c-d1be-4478-8c73-074c68225d4d",
   "metadata": {},
   "source": [
    "### Radiative Model\n",
    "When deriving our physical model, we assumed that we could neglect the influence of thermal radiation. That might not be reasonable, however: the heat transfer due to radiation will be on the order of $1~\\watt$, compared $10~\\watt$ and $100~\\watt$ for the conduction and the convection, respectively, or roughly 1~% of the total heat transfer. In the following section, we also take into account radiation.\n",
    "#### Physical Model\n",
    "According to the [Stefan-Boltzmann Law](https://en.wikipedia.org/wiki/Stefan%E2%80%93Boltzmann_law), the heat flux into a block (i.e. the additive inverse of the heat flux out of a block) will be \n",
    "$$q = -\\varepsilon \\sigma (T^4 - T_a^4),$$\n",
    "where $\\sigma = 5.67 \\times 10^{-8}~\\watt~\\metre^{-2}~\\kelvin^{-4}$ is the Stefan-Boltzmann constant Bouwens (2013), p. 8, and $\\varepsilon \\in [0, 1]$ is the so-called _emissivity_, which depends on e.g. how smooth the surface is. For instance, for polished metals the emissivity can be as low as $10^{-2}$, while for anodised aluminium it can be on the order of unity. \n",
    "\n",
    "Radiation would be an interesting effect to additionally model:\n",
    "- The emissivity depends heavily on e.g. how clean the surface is, and hence probably is not known very well;\n",
    "- Radiation would be the only term in our physical model that is nonlinear in the temperature. Consequently, we would no longer dealing with a _linear_ state space model. However, Turing.jl + DifferentialEquations.jl should be able to deal with it;\n",
    "- The emissivity $\\varepsilon \\in [0, 1]$, and consequently it makes sense to pose a Uniform (or more generally Beta) prior. This is something that would probably get messy if we were trying to deal with this problem analytically or using VB, but for MCMC it should not matter (HMC/NUTS actually requires parameters to have unbounded suppport, but for nice prior distributions this can easily be achieved with a bijector, a bijective transform. Turing does this automatically for us)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975188cb-11e6-4dfb-b5b1-f54698e0cc1d",
   "metadata": {},
   "source": [
    "#### Simulate Data\n",
    "We can easily add the term corresponding to the influence of thermal radiation to the physical model ODEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5021a-6fee-48e8-b902-e9b2c82d2c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emissivities\n",
    "true_Îµ1 = 0.85\n",
    "true_Îµ2 = 0.89\n",
    "true_Îµ3 = 0.92\n",
    "\n",
    "# System of ODEs governing our radiative model\n",
    "function RSSM_lump(dT, T, p, t)\n",
    "    mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, k12, k23, h_a, Îµ1, Îµ2, Îµ3 = p\n",
    "    Ïƒ_sb = 5.67e-8 # Stefan-Boltzmann constant\n",
    "    # Conduction\n",
    "    dT[1] = k12 * (T[2] - T[1]) / mcp_1\n",
    "    dT[2] = (k12 * (T[1] - T[2]) + k23 * (T[3] - T[2])) / mcp_2\n",
    "    dT[3] = k23 * (T[2] - T[3]) / mcp_3\n",
    "    # Convection\n",
    "    dT[1] += h_a * A_1 * (T_a - T[1]) / mcp_1\n",
    "    dT[2] += h_a * A_2 * (T_a - T[2]) / mcp_2\n",
    "    dT[3] += h_a * A_3 * (T_a - T[3]) / mcp_3\n",
    "    # Radiation\n",
    "    dT[1] -= A_1 * Îµ1 * Ïƒ_sb * (T[1]^4 - T_a^4) / mcp_1\n",
    "    dT[2] -= A_2 * Îµ2 * Ïƒ_sb * (T[2]^4 - T_a^4) / mcp_2\n",
    "    dT[3] -= A_3 * Îµ3 * Ïƒ_sb * (T[3]^4 - T_a^4) / mcp_3    \n",
    "    # Input\n",
    "    dT[1] += Î¦(B_1, Ï‰_1, t) / mcp_1\n",
    "    dT[2] += Î¦(B_2, Ï‰_2, t) / mcp_2\n",
    "    dT[3] += Î¦(B_3, Ï‰_3, t) / mcp_3\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d3143-3dee-4f33-af6b-636d724c04de",
   "metadata": {},
   "source": [
    "To get an idea of the effect of radiation, we visualise the true temperature evolution with the observations overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47768da-3440-4014-89f7-7c15a2889b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, true_T_a, true_k12, true_k23, true_h_a, true_Îµ1, true_Îµ2, true_Îµ3] # First known, then unknown parameters\n",
    "RSSM_dynamics_lump = ODEProblem(RSSM_lump, T_0, (time[1], time[end]), true_p)\n",
    "true_sol = solve(RSSM_dynamics_lump, Tsit5(); saveat = Î”, verbose = false)\n",
    "true_Ïƒ = 1e-2\n",
    "y = Array(true_sol) + true_Ïƒ * randn(size(Array(true_sol)))\n",
    "plot_obs = observ_plot(Array(true_sol)', y, time)\n",
    "# savefig(plot_obs, \"Results\\\\Expand\\\\rad_rad_obs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42bd7a-5f39-4bcb-ac20-10eaade6ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmap = state_heatmap(Array(true_sol), time, Î”)\n",
    "# savefig(hmap, \"Results\\\\Expand\\\\rad_rad_obs_heatmap.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ac681-cdb7-42fc-bbf2-7e5c785c2ae0",
   "metadata": {},
   "source": [
    "The influence of radiation appears to be limited: the trajectories do not look very different to those generated by the lumped-element model. This makes sense since we expect the power associated with radiation to be one and two orders of magnitude smaller than those associated with the convection and conduction, respectively. It will be interesting to see whether the inference will be able to \"pick out\" the contribution of the radiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad47e34-4eaa-459f-b8fa-33cd85096cea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Probabilistic Model\n",
    "It is also straightforward to adapt the probabilistic model to take into account the unknown emissivities: we simply pose Beta priors, which have support $[0, 1]$. Typically, we would have a reasonable idea of the emissivity. For instance, for anodised aluminium, we know it is about $0.85$, maybe $\\pm 0.1$. Hence, we choose hyperparameters for our Beta priors such that they have as mean $0.85$ and as standard deviation $0.05$. We determine these hyperparameters using the Method of Moments: we know that if $X \\haslaw \\BetaDist(\\alpha, \\beta)$, then Berkum (2016):\n",
    "$$\\Expectation[X] = \\frac{\\alpha}{\\alpha + \\beta}, \\textrm{ and } \\Variance(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta + 1) (\\alpha + \\beta)^2} = \\Expectation[X] \\cdot \\frac{\\beta}{(\\alpha + \\beta + 1) (\\alpha + \\beta)}.$$\n",
    "If we solve these equations for $\\alpha$ and $\\beta$, we find\n",
    "$$\\alpha = \\left(\\frac{\\Expectation[X] (1 - \\Expectation[X])}{\\Variance(X)} - 1\\right) \\Expectation[X], \\textrm{ and } \\beta = \\left(\\frac{\\Expectation[X] (1 - \\Expectation[X])}{\\Variance(X)} - 1\\right) (1 - \\Expectation[X]).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2bce9f-2d7f-49c6-9f5a-eec0a70741fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Beta distribution hyperparameters to get given mean and standard deviation\n",
    "function calc_beta_params(mean, std)\n",
    "    var = std^2\n",
    "    fac = (mean * (1 - mean) / var - 1)\n",
    "    Î± = fac * mean\n",
    "    Î² = fac * (1 - mean)\n",
    "    return Î±, Î²\n",
    "end\n",
    "\n",
    "# Tell Turing our probabilistic model is based on the radiative model, with normally distributed noise\n",
    "@model function fit_RSSM_lump(data, system, syst_consts)\n",
    "    # Gamma in Distributions.jl is shape-scale, not shape-rate\n",
    "    Ïƒ ~ Gamma(1e-2, 1e0) # E[Ïƒ] = 10^-2, Var(Ïƒ) = 10^-2 \n",
    "    k12 ~ Gamma(1e0, 1e0) # E[k] = 10^0, Var(k) = 10^0\n",
    "    k23 ~ Gamma(1e0, 1e0) \n",
    "    h_a ~ Gamma(1e1, 1e0) # E[h_a] = 10^1, Var(h_a) = 10^1\n",
    "    prior_Î±, prior_Î² = calc_beta_params(0.85, 0.1) # Compute reasonable emissivity prior hyperparameters\n",
    "    Îµ1 ~ Beta(prior_Î±, prior_Î²)\n",
    "    Îµ2 ~ Beta(prior_Î±, prior_Î²)\n",
    "    Îµ3 ~ Beta(prior_Î±, prior_Î²)\n",
    "    T_a, mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, Î” = syst_consts\n",
    "    p = [mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, k12, k23, h_a, Îµ1, Îµ2, Îµ3]\n",
    "    predicted = solve(system, Tsit5(); p = p, saveat = Î”, verbose = false)\n",
    "\n",
    "    for i âˆˆ 1:length(predicted)\n",
    "        data[:, i] ~ MvNormal(predicted[i], Ïƒ^2 * I)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f7c431-08a0-4ecb-861d-dc0a73610aa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inference\n",
    "Finally, we can perform the sampling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d3d7a-df7e-4e39-ba2b-824d5e7f634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rad_rad = fit_RSSM_lump(y, RSSM_dynamics_lump, [true_T_a, true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, Î”]);\n",
    "chain_rad_rad = sample(model_rad_rad, NUTS(0.65), MCMCSerial(), 2500, 3; verbose = false, progress = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47314c8-18a8-4b0f-9772-650e6a61616b",
   "metadata": {},
   "source": [
    "... and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99f26b-3637-4047-998d-3e89ca3a1460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior_samples = Array(sample(chain_rad_rad, 300; replace = false))\n",
    "plot_sol_app = sampled_traj_plot(posterior_samples, true_sol, y, time, Î”, true_p, 5, RSSM_dynamics_lump)\n",
    "# savefig(plot_sol_app, \"Results\\\\Expand\\\\rad_rad_states.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ec109-08fa-4235-bf7c-1f65413976c4",
   "metadata": {},
   "source": [
    "The inference appears to have been highly successful once again: the sampled trajectories are again too close to the true trajectories to see them. \n",
    "\n",
    "We again look at the (approximate) posteriors of the parameters. I did not want 7 plots, so I have not plotted the posterior of the measurement noise standard deviation. It is similar to the one we saw before, suggesting that the radiation term does not cause much more process noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87d301-59da-41be-a1e6-0277b8092b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_k12 = marg_post_plot(:k12, true_k12, chain_rad_rad, 300)\n",
    "p_k23 = marg_post_plot(:k23, true_k23, chain_rad_rad, 200)\n",
    "p_h_a = marg_post_plot(:h_a, true_h_a, chain_rad_rad, 1.5)\n",
    "prior_Î±, prior_Î² = calc_beta_params(0.85, 0.1)\n",
    "p_Îµ1 = marg_post_plot(:Îµ1, true_Îµ1, chain_rad_rad, 8)\n",
    "plot!(p_Îµ1, Beta(prior_Î±, prior_Î²), label = \"Prior\", linestyle = :dash, color = \"black\")\n",
    "p_Îµ2 = marg_post_plot(:Îµ2, true_Îµ2, chain_rad_rad, 8)\n",
    "plot!(p_Îµ2, Beta(prior_Î±, prior_Î²), label = \"Prior\", linestyle = :dash, color = \"black\")\n",
    "p_Îµ3 = marg_post_plot(:Îµ3, true_Îµ3, chain_rad_rad, 8)\n",
    "plot!(p_Îµ3, Beta(prior_Î±, prior_Î²), label = \"Prior\", linestyle = :dash, color = \"black\")\n",
    "plot_marg_post = plot(p_k12, p_k23, p_h_a, p_Îµ1, p_Îµ2, p_Îµ3, size = (1800, 800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(plot_marg_post, \"Results\\\\Expand\\\\rad_rad_params.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd1691-96c6-4436-840d-f93fda348311",
   "metadata": {},
   "source": [
    "The posteriors of all the parameters are very good: peaking near the true value, and quite narrow. Notably, however, the posteriors of the parameters $h_a$, $k_{12}$, and $k_{23}$ are a bit wider than before. Maybe unidentifiability plays a role in this? The posteriors of the emissivities are a little bit taller than the priors, but it seems like we were not able to learn a lot about them from these measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f0aa0-fe3e-478a-85bd-9e9e59983e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corn = cornerplot(posterior_samples[:, 2:end], label = [L\"k_{12}\", L\"k_{23}\", L\"h_a\", L\"Îµ_1\", L\"Îµ_2\", L\"Îµ_3\"], size = (2400, 2400), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(corn, \"Results\\\\Expand\\\\rad_rad_corner.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55116cb-6dd8-46cf-aebf-906039a66ab0",
   "metadata": {},
   "source": [
    "#### Validation\n",
    "We perform a similar validation to before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc25b94-64a9-4f33-ae98-bfbfaa59d78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# System of ODEs to validate the radiative model for prediction\n",
    "function RSSM_lump_val(dT, T, p, t)\n",
    "    mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, t_start_1, t_start_2, t_start_3, t_end_1, t_end_2, t_end_3, T_a, k12, k23, h_a, Îµ1, Îµ2, Îµ3 = p\n",
    "    Ïƒ_sb = 5.67e-8 # Stefan-Boltzmann constant\n",
    "    # Conduction\n",
    "    dT[1] = k12 * (T[2] - T[1]) / mcp_1\n",
    "    dT[2] = (k12 * (T[1] - T[2]) + k23 * (T[3] - T[2])) / mcp_2\n",
    "    dT[3] = k23 * (T[2] - T[3]) / mcp_3\n",
    "    # Convection\n",
    "    dT[1] += h_a * A_1 * (T_a - T[1]) / mcp_1\n",
    "    dT[2] += h_a * A_2 * (T_a - T[2]) / mcp_2\n",
    "    dT[3] += h_a * A_3 * (T_a - T[3]) / mcp_3\n",
    "    # Radiation\n",
    "    dT[1] -= A_1 * Îµ1 * Ïƒ_sb * (T[1]^4 - T_a^4) / mcp_1\n",
    "    dT[2] -= A_2 * Îµ2 * Ïƒ_sb * (T[2]^4 - T_a^4) / mcp_2\n",
    "    dT[3] -= A_3 * Îµ3 * Ïƒ_sb * (T[3]^4 - T_a^4) / mcp_3    \n",
    "    # Input\n",
    "    dT[1] += Î¦_val_smooth(B_1, t_start_1, t_end_1, t) / mcp_1\n",
    "    dT[2] += Î¦_val_smooth(B_2, t_start_2, t_end_2, t) / mcp_2\n",
    "    dT[3] += Î¦_val_smooth(B_3, t_start_3, t_end_3, t) / mcp_3\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20f2bc-fd4f-43a1-89e2-6c3c8068614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, true_T_a, true_k12, true_k23, true_h_a, true_Îµ1, true_Îµ2, true_Îµ3] \n",
    "RSSM_dynamics_lump_val = ODEProblem(RSSM_lump_val, T_0, (time_val[1], time_val[end]), val_p)\n",
    "val_sol = solve(RSSM_dynamics_lump_val, Tsit5(); saveat = Î”_val, verbose = false);\n",
    "p_in = val_input_plot(val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, time_val)\n",
    "p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3 = sampled_traj_val_plot(posterior_samples, val_sol, time_val, Î”_val, val_p, true_Ïƒ, 5, RSSM_dynamics_lump_val)\n",
    "plot_val = plot(p_in, p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3, size = (1800, 800), layout = (2, 3), leftmargin = 10mm, bottommargin = 10mm)\n",
    "# savefig(plot_val, \"Results\\\\Validation\\\\rad_rad_val.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88cfcf-77f0-45bf-bc43-7f809ebd135d",
   "metadata": {},
   "source": [
    "The results of this validation are similar to the previous one. This makes sense, since we are still using the same model to generate the data as to identify the parameters. The residuals are somewhat larger, which makes sense since our parameter posteriors are wider. For some reason, the residuals start to diverge from $t \\approx 300~\\second$: we don't know the cause of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ce505-bf04-47a2-bf8e-62defbf8cb3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inference: Probabilistic Model â‰  Physical Model\n",
    "It would be interesting to know whether neglecting the radiation in the model we use for identification has a significant impact on the quality of the inference, because it can give us some intuition about whether we need to worry about small contributions that we have ignored/have not thought of. Obviously in this way we will not get estimates for the emissivities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d73a98-d66b-410b-bc7b-5db9310e7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rad_nonrad = fit_LSSM_lump(y, LSSM_dynamics_lump, [true_T_a, true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, Î”]);\n",
    "chain_rad_nonrad = sample(model_rad_nonrad, NUTS(0.65), MCMCSerial(), 2500, 3; verbose = false, progress = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be200644-e5d9-464c-a63c-4f7630054fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, true_T_a, true_k12, true_k23, true_h_a]\n",
    "posterior_samples = Array(sample(chain_rad_nonrad, 300; replace = false))\n",
    "plot_sol_app = sampled_traj_plot(posterior_samples, true_sol, y, time, Î”, true_p, 2, LSSM_dynamics_lump)\n",
    "# savefig(plot_sol_app, \"Results\\\\Expand\\\\rad_nonrad_states.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94fec9-a3bc-4988-8023-96f1a83703be",
   "metadata": {},
   "source": [
    "Despite the fact that the generative and identification models differ, the state estimates still appear to be very good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37e08f-e2ed-4cf1-bbb0-270d144fe4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_Ïƒ = marg_post_plot(:Ïƒ, true_Ïƒ, chain_rad_nonrad, 600)\n",
    "p_k12 = marg_post_plot(:k12, true_k12, chain_rad_nonrad, 900)\n",
    "p_k23 = marg_post_plot(:k23, true_k23, chain_rad_nonrad, 600)\n",
    "p_h_a = marg_post_plot(:h_a, true_h_a, chain_rad_nonrad, 14)\n",
    "plot_marg_post = plot(p_Ïƒ, p_k12, p_k23, p_h_a, size = (1200, 800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(plot_marg_post, \"Results\\\\Expand\\\\rad_nonrad_params.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e65e5-6dda-4ef2-a512-f6a7f5488983",
   "metadata": {},
   "source": [
    "On the other hand, we see that the MAP estimates for each parameter is far from the true value. The estimate of the convection coefficient is particularly bad, which I think makes sense because the influence of radiation is most similar to that of convection: this means that convection takes over the role of radiation. Notably, the MAP estimate of the measurement noise standard deviation is about one order of magnitude greater than before. This is caused by the fact that the influences that we do not model behave somewhat like process noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c740f86-f946-470a-bff8-25a26916d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corn = cornerplot(posterior_samples, label = [L\"Ïƒ\", L\"k_{12}\", L\"k_{23}\", L\"h_a\"], size = (1800, 1800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(corn, \"Results\\\\Expand\\\\rad_nonrad_corner.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ef304-2b62-48b1-bbca-b1266b674d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, true_T_a, true_k12, true_k23, true_h_a, true_Îµ1, true_Îµ2, true_Îµ3] \n",
    "RSSM_dynamics_lump_val = ODEProblem(RSSM_lump_val, T_0, (time_val[1], time_val[end]), val_p)\n",
    "val_sol = solve(RSSM_dynamics_lump_val, Tsit5(); saveat = Î”_val, verbose = false);\n",
    "val_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, true_T_a, true_k12, true_k23, true_h_a] \n",
    "p_in = val_input_plot(val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, time_val)\n",
    "p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3 = sampled_traj_val_plot(posterior_samples, val_sol, time_val, Î”_val, val_p, true_Ïƒ, 2, LSSM_dynamics_lump_val)\n",
    "plot_val = plot(p_in, p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3, size = (1800, 800), layout = (2, 3), leftmargin = 10mm, bottommargin = 10mm)\n",
    "# savefig(plot_val, \"Results\\\\Validation\\\\rad_nonrad_val.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b195e4-75bd-42a0-9eef-8bb754124ca1",
   "metadata": {},
   "source": [
    "### Sliced Blocks Model\n",
    "In our [exploration of MCMC](Explore_MCMC.ipynb), as well as the previous examples, we saw that MCMC works well when we use the simple lumped-element model (with blocks with internally constant temperature) for both the physical and the probabilistic model. To derive our governing equations, we implicitly assumed that conduction within blocks occurs at a much higher rate than any other heat transfer process, so that each block has a single temperature throughout. This is only realistic if the internal conduction is much faster than the conduction between blocks. In the following section, we will first discuss how to augment the physical model to allow for temperature gradients within blocks. We will then perform inference on data generated in this way, using two different probabilistic models: the more simple lumped-element model we have used up to now, and the sliced blocks model we propose in this section.\n",
    "#### Physical Model\n",
    "One way we can deal with this is by chopping each block up into (equally sized) slices, which exchange heat between each other. We assume that slices within a block conduct in a known way. This seems sensible because internal conduction coefficients of metals are well-known physical constants (though they may depend on temperature, I think it reasonable to neglect this in the termpature ranges we are dealing with). We only observe the temperature of the central slice of each block, which is also where the external heat gets put into the system (This probably is not a very sensible thing to do in real life...), reusing the input heats from above. Finally, we again neglect the influence of radiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66bc5f-4e63-4e58-9677-789bf5cab36d",
   "metadata": {},
   "source": [
    "#### Simulate Data\n",
    "To perform inference in a reasonable amount of time, we need to use fewer observations. We also have to limit the number of slices in order to keep the computation time down. If you are only interested in seeing the evolution of the temperatures with known parameters, however, it is no problem to use 100 slices per block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c432f-d380-42fe-b257-3a5e6d33dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time horizon\n",
    "sample_size = 25\n",
    "Î” = 20 # Final time t = 500 s\n",
    "time = [Î” * i for i in 0:sample_size] \n",
    "\n",
    "# Create the slices\n",
    "slices_per_block = 5 # This should work for any positive integer (hopefully ðŸ¤ž), including 1. More slices will make everything slower\n",
    "total_slices = 3 * slices_per_block\n",
    "mid_of_block = ceil(Int64, slices_per_block / 2)\n",
    "observed_nodes = [i * slices_per_block + mid_of_block for i in 0:2]\n",
    "# Conduction coefficients\n",
    "int_vs_ext_cond = 1e0\n",
    "true_k1 = 2e1 * slices_per_block * int_vs_ext_cond\n",
    "true_k2 = 2.5e1 * slices_per_block * int_vs_ext_cond\n",
    "true_k3 = 2.2e1 * slices_per_block * int_vs_ext_cond\n",
    "true_ks_int = [true_k1, true_k2, true_k3]\n",
    "\n",
    "# Heat capacities\n",
    "true_Cs = Vector{Float64}(undef, total_slices)\n",
    "true_Cs[1:slices_per_block] .= true_mcp_1 / slices_per_block\n",
    "true_Cs[(slices_per_block + 1):(2 * slices_per_block)] .= true_mcp_2 / slices_per_block\n",
    "true_Cs[(2 * slices_per_block + 1):(3 * slices_per_block)] .= true_mcp_3 / slices_per_block\n",
    "\n",
    "# Slice surface areas\n",
    "true_As = Vector{Float64}(undef, total_slices)\n",
    "true_As[1:slices_per_block] .= true_A_1 / slices_per_block\n",
    "true_As[(slices_per_block + 1):(2 * slices_per_block)] .= true_A_2 / slices_per_block\n",
    "true_As[(2 * slices_per_block + 1):(3 * slices_per_block)] .= true_A_3 / slices_per_block\n",
    "\n",
    "# Initial temperatures\n",
    "T_0_slice = Vector{Float64}(undef, total_slices)\n",
    "T_0_slice[1:slices_per_block] .= T_0[1]\n",
    "T_0_slice[(slices_per_block + 1):(2 * slices_per_block)] .= T_0[2]\n",
    "T_0_slice[(2 * slices_per_block + 1):(3 * slices_per_block)] .= T_0[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb5c73-2e0b-4497-8cee-0544e9ea81d7",
   "metadata": {},
   "source": [
    "Using the true parameters, we can solve this system with DifferentialEquations.jl, and visualise the evolution of the temperatures, latent and observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e85ceb-adf9-4c68-a759-83021fa321cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often \n",
    "function Î¦_cond!(dT, T, ks, Cs, N_s)\n",
    "    dT[1] = ks[1] * (T[2] - T[1]) / Cs[1]\n",
    "    dT[2:(N_s - 1)] .= @. (ks[1:(N_s - 2)] * (T[1:(N_s - 2)] - T[2:(N_s - 1)]) + ks[2:(N_s - 1)] * (T[3:N_s] - T[2:(N_s - 1)])) / Cs[2:(N_s - 1)]\n",
    "    dT[N_s] = ks[N_s - 1] * (T[N_s - 1] - T[N_s]) / Cs[N_s] \n",
    "end\n",
    "\n",
    "function Î¦_conv!(dT, T, h_a, T_a, As, Cs)\n",
    "    dT .+= @. h_a * As * (T_a - T) / Cs\n",
    "end\n",
    "\n",
    "function Î¦_input!(dT, T, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, t, Cs, obs_slices)\n",
    "    dT[obs_slices[1]] += Î¦(B_1, Ï‰_1, t) / Cs[obs_slices[1]]\n",
    "    dT[obs_slices[2]] += Î¦(B_2, Ï‰_2, t) / Cs[obs_slices[2]]\n",
    "    dT[obs_slices[3]] += Î¦(B_3, Ï‰_3, t) / Cs[obs_slices[3]]\n",
    "end\n",
    "\n",
    "function LSSM_slice(dT, T, p, t)\n",
    "    s, obs_slices, Cs, As, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, ks_int, k12, k23, h_a = p\n",
    "    N_s = 3 * s\n",
    "    # Put conduction coefficient between adjacent slices in list\n",
    "    ks = Vector{Any}(undef, N_s - 1) # DifferentialEquations uses automatic differentiation. This converts Float64 to complicated ForwardDiff type. ks must contain elements of both types.\n",
    "    ks[1:(s - 1)] .= ks_int[1]\n",
    "    ks[s] = k12\n",
    "    ks[(s + 1):(2 * s - 1)] .= ks_int[2]\n",
    "    ks[2 * s] = k23\n",
    "    ks[(2 * s + 1):(3 * s - 1)] .= ks_int[3]\n",
    "    \n",
    "    Î¦_cond!(dT, T, ks, Cs, N_s)\n",
    "    Î¦_conv!(dT, T, h_a, T_a, As, Cs)\n",
    "    Î¦_input!(dT, T, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, t, Cs, obs_slices)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fbbbe-ec61-4e90-9e58-7a4763bd3965",
   "metadata": {},
   "source": [
    "Note that if we increase the conduction coefficients within the block, our previous assumption of a homogeneous internal temperature should become more reasonable. Indeed, if we put them at around $10^5$, we essentially see just 3 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24193d-fb80-4860-b2e2-49c793ec9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p_slice = [slices_per_block, observed_nodes, true_Cs, true_As, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, true_T_a, true_ks_int, true_k12, true_k23, true_h_a]\n",
    "LSSM_dynamics_slice = ODEProblem(LSSM_slice, T_0_slice, (time[1], time[end]), true_p_slice)\n",
    "true_sol = solve(LSSM_dynamics_slice, Tsit5(); saveat = Î”, verbose = false)\n",
    "plot_obs = plot(true_sol, title = \"Evolution of All Temperatures\", xlabel = L\"t~(\\textrm{s})\", ylabel = L\"T~(\\textrm{K})\", legend = nothing, xlim = (time[1], time[end]), ylim = (260, 340), size = (1200, 400), bottommargin = 6mm, leftmargin = 6mm)\n",
    "# savefig(plot_obs, \"Results\\\\Expand\\\\sliced_obs.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258df760-c2c8-4f8f-86f9-6114147efca0",
   "metadata": {},
   "source": [
    "I think a heatmap will make the internal temperature differences more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079450b6-c667-4739-be6d-f57afaa3c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmap = state_heatmap(Array(true_sol), time, Î”)\n",
    "# savefig(hmap, \"Results\\\\Expand\\\\sliced_obs_heatmap_slow_fine.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa412372-5cda-4e93-bd2e-d557f33719c5",
   "metadata": {},
   "source": [
    "Recall that we still only observe a single temperature per block (namely the one in the centre slice). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cdb3d-4b9a-4867-abd6-09558be52069",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_obs = Array(true_sol)[observed_nodes, :]\n",
    "true_Ïƒ = 1e-2\n",
    "y = T_obs + true_Ïƒ * randn(size(T_obs))\n",
    "plot_obs = observ_plot(T_obs', y, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcce5d8-d025-42aa-9fe0-fbc2e689ebed",
   "metadata": {},
   "source": [
    "#### Inference: Probabilistic Model â‰  Physical Model\n",
    "Next, we will perform the inference. It is interesting to consider the following: we have generated our data using some contrived physical model. In the previous example, we have subsequently performed inference using the physical model as our probabilistic model. In practice, however, our data will not come from a known physical model. Instead, it will be generated in some complicated way, which is modelled to a certain degree by some known physical model. It therefore feels like cheating to use the equations generating the data as the probabilistic model. \n",
    "\n",
    "Hence, in the next two subsections we will perform data generated using our sliced blocks model, using two different probabilistic models: the lumped-element and sliced blocks models. In this subsection, the probabilistic model is the lumped-element model, so that the physical and probabilistic model differ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f705c4-3e7f-4636-b64b-ed6b2fba5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, true_T_a, true_k12, true_k23, true_h_a] # First known, then unknown parameters\n",
    "LSSM_dynamics_lump = ODEProblem(LSSM_lump, T_0, (time[1], time[end]), true_p)\n",
    "model_slice_lump = fit_LSSM_lump(y, LSSM_dynamics_lump, [true_T_a, true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, Î”]);\n",
    "chain_slice_lump = sample(model_slice_lump, NUTS(0.65), MCMCSerial(), 2500, 3; verbose = false, progress = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98f76f-5448-41e3-a31c-1a60cb697671",
   "metadata": {},
   "source": [
    "We once again plot some sampled trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62aa42-38e2-478b-adb9-5b50285477d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = Array(sample(chain_slice_lump, 100; replace = false))\n",
    "plot_sol_app = sampled_traj_plot(posterior_samples, true_sol, y, time, Î”, true_p, 2, LSSM_dynamics_lump, observed_nodes)\n",
    "# savefig(plot_sol_app, \"Results\\\\Expand\\\\slice_lump_states.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a2a98-8345-48af-ae14-02f8a6e4bc88",
   "metadata": {},
   "source": [
    "Here, like when we used the linear state space model on data that was generated by the model including the influence of radiation, we see that the sampled trajectories are quite close together, but there are systematic error with respect to the true trajectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8155e-1574-4680-b4d0-3453be94ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_Ïƒ = marg_post_plot(:Ïƒ, true_Ïƒ, chain_slice_lump, 40)\n",
    "p_k12 = marg_post_plot(:k12, true_k12, chain_slice_lump, 60)\n",
    "p_k23 = marg_post_plot(:k23, true_k23, chain_slice_lump, 40)\n",
    "p_h_a = marg_post_plot(:h_a, true_h_a, chain_slice_lump, 1)\n",
    "plot_marg_post = plot(p_Ïƒ, p_k12, p_k23, p_h_a, size = (1200, 800), leftmargin = 6mm, bottommargin = 6mm)\n",
    "# savefig(plot_marg_post, \"Results\\\\Expand\\\\slice_lump_params.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05640d6e-9316-4405-9432-b8969a9f06b6",
   "metadata": {},
   "source": [
    "Similarly, the empirical posteriors for the conduction coefficients again show sizable biases. I think this is caused by the fact that the temperature differences at the boundaries of the blocks will be smaller than one would expect if the temperatures were constant within the blocks, because the internal conduction will tend to even things out. This causes the conduction coefficients to be underestimated. The standard deviation of the measurement noise is once again massively overestimated. \n",
    "\n",
    "Playing around with the value of `int_vs_ext_cond`, we see that inference gets worse as we make internal conduction coefficients small. This makes sense, since we expect our assumption to become more reasonable as the internal conduction becomes faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0be04-4afd-44c3-86e8-cb80f5a0cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corn = cornerplot(posterior_samples, label = [L\"Ïƒ\", L\"k_{12}\", L\"k_{23}\", L\"h_a\"], size = (1800, 1800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(corn, \"Results\\\\Expand\\\\slice_lump_corner.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad7e39-312b-49fb-9c94-5ebf9e1a1153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function Î¦_input_val!(dT, T, B_1, B_2, B_3, t_start_1, t_start_2, t_start_3, t_end_1, t_end_2, t_end_3, t, Cs, obs_slices)\n",
    "    dT[obs_slices[1]] += Î¦_val_smooth(B_1, t_start_1, t_end_1, t) / Cs[obs_slices[1]]\n",
    "    dT[obs_slices[2]] += Î¦_val_smooth(B_2, t_start_2, t_end_2, t) / Cs[obs_slices[2]]\n",
    "    dT[obs_slices[3]] += Î¦_val_smooth(B_3, t_start_3, t_end_3, t) / Cs[obs_slices[3]]\n",
    "end\n",
    "\n",
    "function LSSM_slice_val(dT, T, p, t)\n",
    "    s, obs_slices, Cs, As, B_1, B_2, B_3, t_start_1, t_start_2, t_start_3, t_end_1, t_end_2, t_end_3, T_a, ks_int, k12, k23, h_a = p\n",
    "    N_s = 3 * s\n",
    "    # Put conduction coefficient between adjacent slices in list\n",
    "    ks = Vector{Any}(undef, N_s - 1) \n",
    "    ks[1:(s - 1)] .= ks_int[1]\n",
    "    ks[s] = k12\n",
    "    ks[(s + 1):(2 * s - 1)] .= ks_int[2]\n",
    "    ks[2 * s] = k23\n",
    "    ks[(2 * s + 1):(3 * s - 1)] .= ks_int[3]\n",
    "    Î¦_cond!(dT, T, ks, Cs, N_s)\n",
    "    Î¦_conv!(dT, T, h_a, T_a, As, Cs)\n",
    "    Î¦_input_val!(dT, T, B_1, B_2, B_3, t_start_1, t_start_2, t_start_3, t_end_1, t_end_2, t_end_3, t, Cs, obs_slices)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b686cb-6df7-4397-b0b1-6d5e1a9e1f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_p = [slices_per_block, observed_nodes, true_Cs, true_As, val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, true_T_a, true_ks_int, true_k12, true_k23, true_h_a]  \n",
    "LSSM_dynamics_slice_val = ODEProblem(LSSM_slice_val, T_0_slice, (time_val[1], time_val[end]), val_p)\n",
    "val_sol = solve(LSSM_dynamics_slice_val, Tsit5(); saveat = Î”_val, verbose = false);\n",
    "val_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, true_T_a, true_k12, true_k23, true_h_a] \n",
    "p_in = val_input_plot(val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, time_val)\n",
    "p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3 = sampled_traj_val_plot(posterior_samples, val_sol, time_val, Î”_val, val_p, true_Ïƒ, 2, LSSM_dynamics_lump_val, observed_nodes)\n",
    "plot_val = plot(p_in, p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3, size = (1800, 800), layout = (2, 3), leftmargin = 10mm, bottommargin = 10mm)\n",
    "# savefig(plot_val, \"Results\\\\Validation\\\\slice_lump_val.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78d7b9-708d-44df-bfa2-f7f3765a01a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inference: Probabilistic Model = Physical Model\n",
    "<font size='10' color='red'> __WARNING: VERY SLOW ðŸ˜¥__</font>\n",
    "\n",
    "In this subsection, the probabilistic model is the sliced blocks model, so that the physical and probabilistic model coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9213e-59c0-4e09-8c97-5370b78e7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function fit_LSSM_slice(data, system, syst_consts) \n",
    "    # Gamma in Distributions.jl is shape-scale, not shape-rate\n",
    "    Ïƒ ~ Gamma(1e-2, 1e0) # E[Ïƒ] = 10^-2, Var(Ïƒ) = 10^-2 \n",
    "    k12 ~ Gamma(1e0, 1e0) # E[k] = 10^0, Var(k) = 10^0\n",
    "    k23 ~ Gamma(1e0, 1e0) \n",
    "    h_a ~ Gamma(1e1, 1e0) # E[h_a] = 10^1, Var(h_a) = 10^1\n",
    "    # s, mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, k1, k2, k3, Î” = syst_consts\n",
    "    # p = [s, mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, k1, k2, k3, k12, k23, h_a]\n",
    "    s, obs_slices, Cs, As, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, ks_int, Î” = syst_consts\n",
    "    p = [s, obs_slices, Cs, As, B_1, B_2, B_3, Ï‰_1, Ï‰_2, Ï‰_3, T_a, ks_int, k12, k23, h_a]\n",
    "    predicted = solve(system, Tsit5(); p = p, saveat = Î”, verbose = false)\n",
    "    for i âˆˆ 1:length(predicted[1, :])\n",
    "        data[:, i] ~ MvNormal(predicted[obs_slices, i], Ïƒ^2 * I)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d417c526-c210-404e-9dbe-c6ab808760c5",
   "metadata": {},
   "source": [
    "Due to the complexity of the probabilistic model, inference is very slow: e.g. for each sample from the Markov chain, the trajectories (including the latent ones) have to be numerically computed. For that reason, I have decided to only take 1000 samples instead of 2500. On top of that, I only sample from a single Markov chain, whereas in the other examples I used three independent chains. Multiple chains are mostly useful for diagnostic purposes, since by looking at whether the empirical posteriors are roughly equal one can determine whether equilibrium has been reached in the burn-in stage. Moreover, more chains also means more samples (although it would be more efficient to sample from a single chain, since then you only have to throw away a single set of burn-in samples). We simply trust that equilibrium has been reached, and make do with fewer samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9945e7-7e6d-4646-a125-07701a7b0323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_slice_slice = fit_LSSM_slice(y, LSSM_dynamics_slice, [slices_per_block, observed_nodes, true_Cs, true_As, true_B_1, true_B_2, true_B_3, true_Ï‰_1, true_Ï‰_2, true_Ï‰_3, true_T_a, true_ks_int, Î”]);\n",
    "chain_slice_slice = sample(model_slice_slice, NUTS(0.65), MCMCSerial(), 1000, 1; verbose = false, progress = true) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a92c35-b6a3-41b5-b2d9-2d3ef5900b0a",
   "metadata": {},
   "source": [
    "Let's see whether the improvement in the inference was worth the wait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d313a-9717-4fc8-b088-8a11ad7f394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = Array(sample(chain_slice_slice, 100; replace = false))\n",
    "plot_sol_app = sampled_traj_plot(posterior_samples, true_sol, y, time, Î”, true_p_slice, 2, LSSM_dynamics_slice, observed_nodes)\n",
    "# savefig(plot_sol_app, \"Results\\\\Expand\\\\slice_slice_states.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb74ec-2f8e-4d48-b793-a07bf7b7f8e3",
   "metadata": {},
   "source": [
    "That is a good sign: the sampled trajectories once again overlap with the true trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b827c0a-19b4-42f2-a6b9-0e72ec273bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_Ïƒ = marg_post_plot(:Ïƒ, true_Ïƒ, chain_slice_slice, 50)\n",
    "p_k12 = marg_post_plot(:k12, true_k12, chain_slice_slice, 80)\n",
    "p_k23 = marg_post_plot(:k23, true_k23, chain_slice_slice, 60)\n",
    "p_h_a = marg_post_plot(:h_a, true_h_a, chain_slice_slice, 1.25)\n",
    "plot_marg_post = plot(p_Ïƒ, p_k12, p_k23, p_h_a, size = (1200, 800), leftmargin = 6mm, bottommargin = 6mm)\n",
    "# savefig(plot_marg_post, \"Results\\\\Expand\\\\slice_slice_params.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fca7c-43c5-4df4-af49-aa4207b191e7",
   "metadata": {},
   "source": [
    "Indeed, the MAP estimates for the parameters is very close to the true values, although the variance is somewhat larger than when we generated data with the lumped-element model. Using the sliced blocks model for the identification has significantly reduced the estimate for the standard deviation of the measurement noise, but it is still quite large. It therefore might be interesting to investigate what the sources of this are: by dealing with these it could be possible to improve the identification of the parameters of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5531d9-c763-4e73-9473-98a8fbf04056",
   "metadata": {},
   "outputs": [],
   "source": [
    "corn = cornerplot(Array(posterior_samples), label = [L\"Ïƒ\", L\"k_{12}\", L\"k_{23}\", L\"h_a\"], size = (1800, 1800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(corn, \"Results\\\\Expand\\\\slice_slice_corner.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695e298-3eec-4971-861f-b73e855e45be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_p = [slices_per_block, observed_nodes, true_Cs, true_As, val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, true_T_a, true_ks_int, true_k12, true_k23, true_h_a]  \n",
    "LSSM_dynamics_slice_val = ODEProblem(LSSM_slice_val, T_0_slice, (time_val[1], time_val[end]), val_p)\n",
    "val_sol = solve(LSSM_dynamics_slice_val, Tsit5(); saveat = Î”_val, verbose = false);\n",
    "p_in = val_input_plot(val_B_1, val_B_2, val_B_3, val_t_start_1, val_t_start_2, val_t_start_3, val_t_end_1, val_t_end_2, val_t_end_3, time_val)\n",
    "p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3 = sampled_traj_val_plot(posterior_samples, val_sol, time_val, Î”_val, val_p, true_Ïƒ, 2, LSSM_dynamics_slice_val, observed_nodes)\n",
    "plot_val = plot(p_in, p_out_traj, p_out_error, h_mse_1, h_mse_2, h_mse_3, size = (1800, 800), layout = (2, 3), leftmargin = 10mm, bottommargin = 10mm)\n",
    "# savefig(plot_val, \"Results\\\\Validation\\\\slice_slice_val.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836957b0-d76c-4986-8b1c-9bd95d1d42c4",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "- Abramovich, F., & Ritov, Y. (2013).  _Statistical Theory: A Concise Introduction_. CRC Press.\n",
    "- Murphy, K. (2022). _Probabilistic Machine Learning: An Introduction_. URL: [https://github.com/probml/pml-book](https://github.com/probml/pml-book).\n",
    "- Clercx, H. (2015). _Lecture Notes â€˜Physics of Transport Phenomenaâ€™_. ISBN: 9788578110796\n",
    "- Carter, A. (2000). _Classical and Statistical Thermodynamics_. Pearson. \n",
    "- Cox, A., van de Laar, T., & de Vries, B. (2019). _ForneyLab: A factor graph approach to automated design of Bayesian signal processing algorithms_. DOI: [10.1016/j.ijar.2018.11.002](https://doi.org/10.1016/j.ijar.2018.11.002). URL: [https://github.com/biaslab/ForneyLab.jl](https://github.com/biaslab/ForneyLab.jl).\n",
    "- SÃ¤rkkÃ¤, S. (2013). _Bayesian Filtering and Smoothing_. Cambridge University Press. ISBN: 9781482211849. DOI: [10.1080/00401706.1963.10490114](https://doi.org/10.1080/00401706.1963.10490114).\n",
    "- Bagaev, D., & de Vries, B. (2021). _ReactiveMP.jl: a Julia package for automatic Bayesian inference on a factor graph with reactive message passing_. URL: [https://github.com/biaslab/ReactiveMP.jl/releases/tag/v1.3.1](https://github.com/biaslab/ReactiveMP.jl/releases/tag/v1.3.1).\n",
    "- Lutinnen, J. (2013). 'Fast variational Bayesian linear state-space model'. In: _European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD)_. Vol. 8188. pp. 305-320. ISBN: 9783642409875. DOI: [10.1007/978-3-642-40988-2_20](https://doi.org/10.1007/978-3-642-40988-2_20).\n",
    "- Betancourt, M. (2017). _A Conceptual Introduction to Hamiltonian Monte Carlo_. DOI: [10.48550/ARXIV.1701.02434](https://doi.org/10.48550/ARXIV.1701.02434). arXiv: [1701.02434](http://arxiv.org/abs/1701.02434).\n",
    "- Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). 'Equation of State Calculations by Fast Computing Machines'. In: _The Journal of Chemical Physics 21.6_. pp. 1087-1092. ISSN: 00219606. DOI: [10.1063/1.1699114](https://doi.org/10.1063/1.1699114).\n",
    "- Robert, C. P., & Casella, G. (2004). _Monte Carlo Statistical Methods_. Springer Texts in Statistics. ISBN: 978-1-4419-1939-7. DOI: [10.1007/978-1-4757-4145-2](https://doi.org/10.1007/978-1-4757-4145-2). URL: [http://link.springer.com/10.1007/978-1-4757-4145-2](http://link.springer.com/10.1007/978-1-4757-4145-2).\n",
    "- Ge, H., Xu, K., & Ghahramani, Z. (2018). 'Turing: a language for flexible probabilistic inference'. In: _International Conference on Artificial Intelligence and Statistics (AISTATS)_. pp. 1682-1690. URL: [http://proceedings.mlr.press/v84/ge18b.html](http://proceedings.mlr.press/v84/ge18b.html)\n",
    "- Rackaukas, C., & Nie, Q. (2017). 'DifferentialEquations.jlâ€“a performant and feature-rich ecosystem for solving differential equations in julia'. In: _Journal of Open Research Software 5.1_. p. 15\n",
    "- Bouwens, R. E. A., de Groot, P. A. M., Kranendonk, W., van Lune, P., Prop - van den Berg, C. M., van Riswick, J. A. M. H., & Westra, J. J. (2013). _BINAS_. 6th ed. Noordhoff Uitgevers. ISBN: 9789001817497. URL: [https://view.publitas.com/noordhoff-voortgezet-onderwijs-groningen/binas-6e-ed-havo-vwo-informatieboek-9789001817497/page/2-3](https://view.publitas.com/noordhoff-voortgezet-onderwijs-groningen/binas-6e-ed-havo-vwo-informatieboek-9789001817497/page/2-3)\n",
    "- Berkum, E. E. M., & Di Bucchianico, A (2016). _Statistical Compendium_. Eindhoven University of Technology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119f29c-d802-4fe5-96e1-94f05cd5fa9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
