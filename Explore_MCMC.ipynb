{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c11776",
   "metadata": {},
   "source": [
    "# Internship Finn Sherry @ Sioux Mathware\n",
    "\n",
    "---\n",
    "\n",
    "# Bayesian grey-box system identification for thermal effects: MCMC using Turing\n",
    "In this notebook, we apply the MCMC using [Turing.jl](https://github.com/TuringLang/Turing.jl) combined with [DifferentialEquations.jl](https://github.com/SciML/DifferentialEquations.jl). The goal is to assess whether MCMC is a method that is usable for the rest of my project. This notebook continues on from the Markov Chain Monte Carlo part of my [main (Julia) notebook](sysid-thermal-AR.ipynb). Turing and DifferentialEquations are completely composable. This means that we can just plug in the differential equations governing the dynamics of our system.\n",
    "\n",
    "Last update: 27-07-2022\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{\\boldsymbol{\\mathrm{#1}}}$\n",
    "$\\newcommand{\\covec}[1]{\\hat{\\vec{#1}}}$\n",
    "$\\newcommand{\\mat}[1]{\\boldsymbol{\\mathrm{#1}}}$\n",
    "$\\newcommand{\\inv}[1]{#1^{-1}}$\n",
    "$\\newcommand{\\Expectation}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Variance}{\\mathbb{V}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088cb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing, DifferentialEquations, Random, LinearAlgebra, StatsBase # Computational\n",
    "using Measures, LaTeXStrings, StatsPlots # Formatting\n",
    "Random.seed!(987654321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9406f4",
   "metadata": {},
   "source": [
    "## Physical Model\n",
    "Here, we first define the differential equations that govern our system. To start off with, we have 3 blocks, with unknown parameters $k_{12}, k_{23}, h_a$, whose temperatures are governed by\n",
    "$$\\frac{d}{dt}\\begin{pmatrix} m_1 c_{p, 1} T_1 \\\\ m_2 c_{p, 2} T_2 \\\\ m_3 c_{p, 3} T_3 \\end{pmatrix} = \n",
    "\\begin{pmatrix} -k_{12} & k_{12} & 0 \\\\ k_{12} & -(k_{12} + k_{23}) & k_{23} \\\\ 0 & k_{23} & -k_{23} \\end{pmatrix} \\begin{pmatrix} T_1 \\\\ T_2 \\\\ T_3 \\end{pmatrix} + \\begin{pmatrix} \\Phi_1 \\\\ \\Phi_2 \\\\ \\Phi_3 \\end{pmatrix} +\n",
    "h_a \\begin{pmatrix} A_1 (T_a - T_1) \\\\ A_2 (T_a - T_2) \\\\ A_3 (T_a - T_3) \\end{pmatrix}.$$\n",
    "So for the time being we model convection as being linear in the temperature difference. This is called [Newton's Law of Cooling](https://en.wikipedia.org/wiki/Newton's_law_of_cooling), and appears to be decent for forced convection.\n",
    "We completely ignore radiation, since I think its influence is negligible. Using Stefan-Boltzmann law, we can estimate the radiative power over the surface as the blocks to be on the order of $10^2~\\text{W}/\\text{m}^2$. If we are dealing with metals, we can achieve values of $m c_p \\approx 10^3~\\text{J}/\\text{K}$ (as we choose later on), with masses on the order of $10^0~\\text{kg}$, or volumes on the order of $10^{-3}~\\text{m}^3$. This suggests a typical surface area of $10^{-2}~\\text{m}^2$, so that the net heat transfer due to radiation is roughly $10^0~\\text{W}$. Our other heat terms are all $10^2~\\text{W}$, so the contribution of radiation is comparatively small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15621221",
   "metadata": {},
   "source": [
    "## Simulate Data\n",
    "To get a feel for the dynamics, we will first visualise the evolution of the temperatures in our system. We should already get quite interesting dynamic behaviour, since we have a lot of coupling going on, as well as input heats which can be made arbitrarily complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Φ(B, ω, t) = B * sin(ω * t)\n",
    "\n",
    "function LSSM_lump(dT, T, p, t)\n",
    "    mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, ω_1, ω_2, ω_3, T_a, k12, k23, h_a = p\n",
    "    # Conduction\n",
    "    dT[1] = k12 * (T[2] - T[1]) / mcp_1\n",
    "    dT[2] = (k12 * (T[1] - T[2]) + k23 * (T[3] - T[2])) / mcp_2\n",
    "    dT[3] = k23 * (T[2] - T[3]) / mcp_3\n",
    "    # Convection\n",
    "    dT[1] += h_a * A_1 * (T_a - T[1]) / mcp_1\n",
    "    dT[2] += h_a * A_2 * (T_a - T[2]) / mcp_2\n",
    "    dT[3] += h_a * A_3 * (T_a - T[3]) / mcp_3\n",
    "    # Input\n",
    "    dT[1] += Φ(B_1, ω_1, t) / mcp_1\n",
    "    dT[2] += Φ(B_2, ω_2, t) / mcp_2\n",
    "    dT[3] += Φ(B_3, ω_3, t) / mcp_3\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7664a56",
   "metadata": {},
   "source": [
    "Next, we define all the true system parameters, as well as our observations (the measurement error $\\sigma$ is assumed to be unknown in this example). The chosen parameter values are not necessarily particularly realistic.\n",
    "\n",
    "[comment]: # (To reduce warnings from DifferentialEquations.jl, I have to increase the step size. In order to keep the same physics, I multiply the heat capacity $m c_p$, which represents the amount of heat needed to increase the temperature of a block by a certain temperature, by the factor I want to slow it down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time horizon\n",
    "sample_size = 100\n",
    "Δ = 5\n",
    "time = [Δ * i for i in 0:sample_size]\n",
    "# Constants of blocks\n",
    "true_mcp_1 = 2e3\n",
    "true_mcp_2 = 1.5e3\n",
    "true_mcp_3 = 2.5e3\n",
    "true_A_1 = 1.\n",
    "true_A_2 = 1.\n",
    "true_A_3 = 1.\n",
    "# Input heat parameters\n",
    "true_B_1 = 300.\n",
    "true_B_2 = -300.\n",
    "true_B_3 = 150.\n",
    "true_ω_1 = 3e-2\n",
    "true_ω_2 = 2.4e-2\n",
    "true_ω_3 = 3.8e-2\n",
    "# Temperatures\n",
    "true_T_a = 290.\n",
    "T_0 = [330., 270., 310.]\n",
    "# Unknown constants\n",
    "true_k12 = 5.\n",
    "true_k23 = 4.\n",
    "true_h_a = 2.\n",
    "true_p = [true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_ω_1, true_ω_2, true_ω_3, true_T_a, true_k12, true_k23, true_h_a] # First known, then unknown parameters\n",
    "# Solve the system numerically using DifferentialEquations.jl\n",
    "LSSM_dynamics_lump = ODEProblem(LSSM_lump, T_0, (time[1], time[end]), true_p)\n",
    "true_sol = solve(LSSM_dynamics_lump, Tsit5(); saveat = Δ, verbose = false)\n",
    "# Get measurement data\n",
    "true_σ = 1.\n",
    "y = Array(true_sol) + true_σ * randn(size(Array(true_sol)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f902e",
   "metadata": {},
   "source": [
    "We visualise the true solution as well as the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(true_sol, xlim = (time[1], time[end]), ylim = (260, 340), linecolors = [\"red\" \"blue\" \"orange\"], labels = [L\"$T_1$ true\" L\"$T_2$ true\" L\"$T_3$ true\"], xlabel = L\"t~(\\textrm{s})\", ylabel = L\"T~(\\textrm{K})\", size = (1200, 400), bottommargin = 6mm, leftmargin = 6mm)\n",
    "scatter!(true_sol.t, y', markercolors = [\"red\" \"blue\" \"orange\"], labels = [L\"$T_1$ observed\" L\"$T_2$ observed\" L\"$T_3$ observed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c99e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_axis = Vector{String}(undef, sample_size + 1)\n",
    "time_axis .= \"\"\n",
    "steps = 5\n",
    "step = div(sample_size, steps)\n",
    "for i ∈ 0:steps\n",
    "    time_axis[i * step + 1] = string(i * step * Δ)\n",
    "end\n",
    "heatmap(Array(true_sol), xticks = (1:(sample_size + 1), time_axis), xlabel = L\"t~(\\textrm{s})\", ylabel = \"Block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3faed3",
   "metadata": {},
   "source": [
    "## Probabilistic Model\n",
    "To perform the approximate inferencing, we use Turing.jl. We need to define our model, which includes a bunch of fairly uninformative priors. Since physically it makes sense for $k_{12}, k_{23}, h_a > 0$ (otherwise heat would flow from cold places to warm places...), I decided to give all of them Gamma priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function fit_LSSM_lump(data, system, syst_consts)\n",
    "    # Gamma in Distributions.jl is shape-scale, not shape-rate\n",
    "    σ ~ Gamma(1e0, 1e0) # E[σ] = 10^0, Var(σ) = 10^0\n",
    "    k12 ~ Gamma(1e0, 1e0) # E[k] = 10^0, Var(k) = 10^0\n",
    "    k23 ~ Gamma(1e0, 1e0)\n",
    "    h_a ~ Gamma(1e0, 1e0) # E[h_a] = 10^0, Var(h_a) = 10^0\n",
    "    T_a, mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, ω_1, ω_2, ω_3, Δ = syst_consts\n",
    "    p = [mcp_1, mcp_2, mcp_3, A_1, A_2, A_3, B_1, B_2, B_3, ω_1, ω_2, ω_3, T_a, k12, k23, h_a]\n",
    "    predicted = solve(system, Tsit5(); p = p, saveat = Δ, verbose = false)\n",
    "\n",
    "    for i ∈ 1:length(predicted)\n",
    "        data[:, i] ~ MvNormal(predicted[i], σ^2 * I)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c137ed",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Finally, we can perform the sampling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4feeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lump = fit_LSSM_lump(y, LSSM_dynamics_lump, [true_T_a, true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_ω_1, true_ω_2, true_ω_3, Δ]);\n",
    "chain_lump = sample(model_lump, NUTS(0.65), MCMCSerial(), 2500, 3; verbose = false, progress = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c45bc65",
   "metadata": {},
   "source": [
    "In order to plot the results, we sample from the Markov chain. For each sampled parameter, we generate a trajectory using our differential equation, which we overlay on the same plot. This shows that for each of the samples the resulting dynamics is not too far off from the real dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc6705-b271-4268-9dd9-658ba048a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sampled_traj_plot(params_samples, solution, data, horizon, time_step, all_params, id_count, system, obs_slices = nothing)\n",
    "    p_traj = plot(; legend = true, xlim = (horizon[1], horizon[end]), ylim = (260, 340), ylabel = L\"T~(\\textrm{K})\", size = (1200, 400), bottommargin = 6mm, leftmargin = 6mm)\n",
    "    params_cur = copy(all_params)\n",
    "    for (i, params_row) ∈ enumerate(eachrow(params_samples))\n",
    "        params_cur[(end - id_count):end] .= params_row[(end - id_count):end]\n",
    "        traj_cur = solve(system, Tsit5(); p = params_cur, saveat = time_step)\n",
    "        if size(Array(traj_cur))[1] == 3\n",
    "            plot!(p_traj, traj_cur; alpha = 0.05, linecolors = [\"red\" \"blue\" \"orange\"], label = \"\")\n",
    "        else\n",
    "            plot!(p_traj, horizon, Array(traj_cur)[obs_slices, :]'; alpha = 0.05, linecolors = [\"red\" \"blue\" \"orange\"], label = \"\")\n",
    "        end\n",
    "    end\n",
    "    if obs_slices == nothing\n",
    "        plot!(p_traj, solution, linecolors = [\"red\" \"blue\" \"orange\"], linewidth = 1, labels = [L\"T_1\" L\"T_2\" L\"T_3\"]) \n",
    "    else \n",
    "        plot!(p_traj, horizon, Array(solution)[obs_slices, :]', linecolors = [\"red\" \"blue\" \"orange\"], linewidth = 1, labels = [L\"T_1\" L\"T_2\" L\"T_3\"]) \n",
    "    end\n",
    "    scatter!(p_traj, horizon, data', xlabel = L\"t~(\\textrm{s})\", markercolors = [\"red\" \"blue\" \"orange\"], label = \"\")\n",
    "    return p_traj\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = Array(sample(chain_lump, 300; replace = false))\n",
    "plot_sol_app = sampled_traj_plot(posterior_samples, true_sol, y, time, Δ, true_p, 2, LSSM_dynamics_lump)\n",
    "# savefig(plot_sol_app, \"Results/Explore/MCMC/state_evolution_small_noise.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fef1b3-11d4-4939-9d9b-aa0180d56954",
   "metadata": {},
   "source": [
    "Let's have a look at the (approximate) posteriors of the parameters. We will quantify the quality of the estimates by calculating the MSE of the posteriors with respect to the true value. The bias-variance decomposition makes this an easy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821d8a5-207a-464b-9515-0de53fc86291",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean(θ::Symbol, chain) = summarize(chain[[θ]]).nt[:mean][1]\n",
    "get_var(θ::Symbol, chain) = (summarize(chain[[θ]]).nt[:std])[1]^2\n",
    "sci_not(value, sigdigits = 2) = replace(\"$(round(value, sigdigits = sigdigits))\", r\"e(-?\\d+)\" => s\"\\\\times 10^{\\1}\")\n",
    "MSE(true_θ, post_mean, post_var) = (post_mean - true_θ)^2 + post_var\n",
    "\n",
    "function MSE_string(θ::Symbol, true_θ, chain)\n",
    "    mean_θ = get_mean(θ, chain)\n",
    "    var_θ = get_var(θ, chain)\n",
    "    MSE_θ = MSE(true_θ, mean_θ, var_θ)\n",
    "    label = latexstring(\"\\\\textrm{MSE} = \" * sci_not(MSE_θ))\n",
    "    return label\n",
    "end\n",
    "\n",
    "function marg_post_plot(θ::Symbol, true_θ, chain, ymax = nothing)\n",
    "    if ymax == nothing\n",
    "        p_θ = density(chain[[θ]], title = L\"%$θ\", label = \"\", legend = true)      \n",
    "    else\n",
    "        p_θ = density(chain[[θ]], title = L\"%$θ\", label = \"\", legend = true, ylim = (0, ymax))\n",
    "    end\n",
    "    vline!(p_θ, [true_θ], label = L\"True $%$θ$\")\n",
    "    MSE_label = MSE_string(θ, true_θ, chain)\n",
    "    annotate!(p_θ, (0.05, 0.95), (MSE_label, 12, :left))\n",
    "    return p_θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_σ = marg_post_plot(:σ, true_σ, chain_lump, 12)\n",
    "p_k12 = marg_post_plot(:k12, true_k12, chain_lump, 8)\n",
    "p_k23 = marg_post_plot(:k23, true_k23, chain_lump, 5)\n",
    "p_h_a = marg_post_plot(:h_a, true_h_a, chain_lump, 12)\n",
    "plot_marg_post = plot(p_σ, p_k12, p_k23, p_h_a, size = (1200, 800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(plot_marg_post, \"Results/Explore/MCMC/marg_post_dists_small_noise.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab171517-de00-4b28-9eeb-39d8eada59a6",
   "metadata": {},
   "source": [
    "On this fairly complicated physical model, MCMC appears to perform well. \n",
    "\n",
    "Next, we consider how changing the variance of the measurement noise affects the performance of the inference. We start by increasing the variance of the noise by a factor 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ac83d-7188-415c-8902-554c68d92425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get measurement data\n",
    "true_σ = 10.\n",
    "y = Array(true_sol) + true_σ * randn(size(Array(true_sol)))\n",
    "model_lump = fit_LSSM_lump(y, LSSM_dynamics_lump, [true_T_a, true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_ω_1, true_ω_2, true_ω_3, Δ]);\n",
    "chain_lump = sample(model_lump, NUTS(0.65), MCMCSerial(), 2500, 3; verbose = false, progress = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cddba-c0b8-4dde-97d0-f4b2263945be",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = Array(sample(chain_lump, 300; replace = false))\n",
    "plot_sol_app = sampled_traj_plot(posterior_samples, true_sol, y, time, Δ, true_p, 2, LSSM_dynamics_lump)\n",
    "# savefig(plot_sol_app, \"Results/Explore/MCMC/state_evolution_big_noise.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477923e2-e853-458b-b6b1-d09655826059",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_σ = marg_post_plot(:σ, true_σ, chain_lump, 1.25)\n",
    "p_k12 = marg_post_plot(:k12, true_k12, chain_lump, 0.8)\n",
    "p_k23 = marg_post_plot(:k23, true_k23, chain_lump, 0.6)\n",
    "p_h_a = marg_post_plot(:h_a, true_h_a, chain_lump, 1.25)\n",
    "plot_marg_post = plot(p_σ, p_k12, p_k23, p_h_a, size = (1200, 800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(plot_marg_post, \"Results/Explore/MCMC/marg_post_dists_big_noise.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209011e-8648-4631-a5be-6d358ca4e621",
   "metadata": {},
   "source": [
    "For each parameter, the increase in the variance leads to a roughly proportional increase in the MSE (i.e. the MSE increases by an order $10^2$).\n",
    "\n",
    "Finally, we test what happens when there is no measurement noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e9a75-ba07-49f3-a4d8-fa1bc5fdecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get measurement data\n",
    "true_σ = 0.\n",
    "y = Array(true_sol) + true_σ * randn(size(Array(true_sol)))\n",
    "model_lump = fit_LSSM_lump(y, LSSM_dynamics_lump, [true_T_a, true_mcp_1, true_mcp_2, true_mcp_3, true_A_1, true_A_2, true_A_3, true_B_1, true_B_2, true_B_3, true_ω_1, true_ω_2, true_ω_3, Δ]);\n",
    "chain_lump = sample(model_lump, NUTS(0.65), MCMCSerial(), 2500, 3; verbose = false, progress = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e787713-6981-4b60-bc9c-a2fbbad7603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = Array(sample(chain_lump, 300; replace = false))\n",
    "plot_sol_app = sampled_traj_plot(posterior_samples, true_sol, y, time, Δ, true_p, 2, LSSM_dynamics_lump)\n",
    "# savefig(plot_sol_app, \"Results/Explore/MCMC/state_evolution_no_noise.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2490871-26fe-49c3-b0fa-cdb0a98e482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_σ = marg_post_plot(:σ, true_σ, chain_lump, 600)\n",
    "p_k12 = marg_post_plot(:k12, true_k12, chain_lump, 500)\n",
    "p_k23 = marg_post_plot(:k23, true_k23, chain_lump, 250)\n",
    "p_h_a = marg_post_plot(:h_a, true_h_a, chain_lump, 700)\n",
    "plot_marg_post = plot(p_σ, p_k12, p_k23, p_h_a, size = (1200, 800), leftmargin = 8mm, bottommargin = 6mm)\n",
    "# savefig(plot_marg_post, \"Results/Explore/MCMC/marg_post_dists_no_noise.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5825f2-a2db-4e85-8c17-66f4d8eac230",
   "metadata": {},
   "source": [
    "Overall, the inference has performed well. Only the posterior of the measurement noise $\\sigma$ is not very good. This might be caused by the fact that I used a Gamma prior, while $\\sigma = 0$ is not in the support of the Gamma distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab7d95-27f4-4100-b0ef-976f3355148f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "From these experiments, it seems like MCMC, implemented by combining Turing.jl with DifferentialEquations.jl, is a good approximate technique for performing Bayesian inference:\n",
    "- The method is fairly general, which means that it should not be necessary to develop (parts of) software packages;\n",
    "- The method seems to be fairly insensitive to how informative the priors are, although this might change if we were to introduce some process noise;\n",
    "- The accuracy (in terms of MSE) of the inference seems to scale with the measurement noise in a roughly linear way.\n",
    "\n",
    "MCMC is fairly slow however, and so it would not be suitable for online parameter estimation. It would be more suited to occasional calibrations. This is not a problem for my project, however. Consequently, during the rest of this project we will work with MCMC. We continue in the main [Julia Jupyter notebook](sysid-thermal-AR.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
