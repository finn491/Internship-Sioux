{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b7efb9-0533-4385-8077-9897d1eb37d6",
   "metadata": {},
   "source": [
    "# Internship Finn Sherry @ Sioux Mathware\n",
    "\n",
    "---\n",
    "\n",
    "# Bayesian grey-box system identification for thermal effects: UKF using ForneyLab\n",
    "In this notebook, we will try to apply an unscented Kalman filter (UKF), for which we use [ForneyLab.jl](https://github.com/biaslab/ForneyLab.jl), a package for Julia developed by [BIASlab](https://biaslab.github.io/).\n",
    "\n",
    "Last update: 27-07-2022\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{\\boldsymbol{\\mathrm{#1}}}$\n",
    "$\\newcommand{\\covec}[1]{\\hat{\\vec{#1}}}$\n",
    "$\\newcommand{\\mat}[1]{\\boldsymbol{\\mathrm{#1}}}$\n",
    "$\\newcommand{\\inv}[1]{#1^{-1}}$\n",
    "$\\newcommand{\\given}{\\, \\vert \\,}$\n",
    "$\\newcommand{\\problaw}[1]{p(#1)}$\n",
    "$\\newcommand{\\Expectation}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Variance}{\\mathbb{V}}$\n",
    "$\\newcommand{\\Geometric}{\\textrm{Geom}}$\n",
    "$\\newcommand{\\NegBin}{\\textrm{NB}}$\n",
    "$\\newcommand{\\Poisson}{\\textrm{Pois}}$\n",
    "$\\newcommand{\\Bernoulli}{\\textrm{Bern}}$\n",
    "$\\newcommand{\\Uniform}{\\textrm{Uni}}$\n",
    "$\\newcommand{\\NormDist}{\\mathcal{N}}$\n",
    "$\\newcommand{\\GammaDist}{\\textrm{Gamma}}$\n",
    "$\\newcommand{\\ExpDist}{\\textrm{Exp}}$\n",
    "$\\newcommand{\\Uniform}{\\textrm{Uniform}}$\n",
    "$\\newcommand{\\Binomial}{\\textrm{Binom}}$\n",
    "$\\newcommand{\\BetaDist}{\\textrm{Beta}}$\n",
    "$\\newcommand{\\BetaFunc}{\\textrm{B}}$\n",
    "$\\newcommand{\\setify}[1]{\\mathbb{#1}}$\n",
    "$\\newcommand{\\NatSet}{\\setify{N}}$\n",
    "$\\newcommand{\\IntSet}{\\setify{Z}}$\n",
    "$\\newcommand{\\RealSet}{\\setify{R}}$\n",
    "$\\newcommand{\\CompSet}{\\setify{C}}$\n",
    "$\\newcommand{\\QuatSet}{\\setify{H}}$\n",
    "$\\newcommand{\\FieldSet}{\\setify{K}}$\n",
    "$\\newcommand{\\define}{:=}$\n",
    "$\\newcommand{\\enifed}{=:}$\n",
    "$\\newcommand{\\loss}{\\ell}$\n",
    "$\\newcommand{\\risk}{\\textrm{R}}$\n",
    "$\\newcommand{\\MSE}{\\textrm{MSE}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert #2 \\rVert}$\n",
    "$\\newcommand{\\InnerProduct}[2]{\\left( #1 , #2 \\right)}$\n",
    "$\\newcommand{\\kilogram}{\\textrm{kg}}$\n",
    "$\\newcommand{\\metre}{\\textrm{m}}$\n",
    "$\\newcommand{\\watt}{\\textrm{W}}$\n",
    "$\\newcommand{\\joule}{\\textrm{J}}$\n",
    "$\\newcommand{\\kelvin}{\\textrm{K}}$\n",
    "$\\newcommand{\\second}{\\textrm{s}}$\n",
    "$\\newcommand{\\centi}{\\textrm{c}}$\n",
    "$\\newcommand{\\bigO}{\\mathcal{O}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c344a6f-9fa1-41c1-ac80-c8d89de6a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForneyLab, LinearAlgebra, Random # Computational\n",
    "using StatsPlots, LaTeXStrings, Measures # Formatting\n",
    "rng  = MersenneTwister(987654321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847935b-9156-4447-9104-c7de1a987275",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Physical Model\n",
    "To get started, we reduce the system to 1 material with 1 temperature sensor:\n",
    "$$ T_{n + 1} = \\underbrace{\\left(1 - \\Delta t \\frac{h_a A_1}{m_1 c_{p, 1}}\\right)}_{\\theta} T_n + \\underbrace{\\Delta t \\frac{h_a A_1}{m_1 c_{p, 1}}}_{1 - \\theta} T_a \\, , $$\n",
    "Of course now there is neither conduction nor radiation. We also do not put any heat into the system. We can view this system as an autoregressive model with exogenous input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24010704-0b6f-4864-b857-2e474eb0c57d",
   "metadata": {},
   "source": [
    "## Simulate Data\n",
    "Let's start by visualising the system dynamics first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb77950-5b37-4a5d-a73f-2fc7ffb283e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time horizon\n",
    "N = 200\n",
    "Δt = 5e1\n",
    "time = [n * Δt for n in 0:N]\n",
    "# Material properties\n",
    "m = 0.6\n",
    "cp = 1.5e3\n",
    "A = 5e-2\n",
    "# Temperature state\n",
    "T_ = zeros(N + 1)\n",
    "# Known temperatures\n",
    "T_a = 297.\n",
    "T_[1] = 255.\n",
    "# Unknown parameters\n",
    "h_a = 10.\n",
    "# Transition coefficient\n",
    "real_θ = 1 - Δt * h_a * A / (m * cp)\n",
    "# Simulate evolution\n",
    "for n = 1:N\n",
    "    T_[n + 1] = real_θ * T_[n] + (1 - real_θ) * T_a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16975c1f-117c-4356-b0af-9f0250409875",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plot(time, T_, ylabel = L\"T~(\\textrm{K})\", xlabel = L\"t~(\\textrm{s})\", label = L\"T_1\", xlim = (time[1], time[end]), ylim = (250, 300), rightmargin = 6mm)\n",
    "hline!(p1, [T_a], label = L\"T_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2d4f4-19cd-4652-957e-0d3aa95563d3",
   "metadata": {},
   "source": [
    "This is reasonable: we see that the temperature in the block equilibrates to the ambient temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1189e4ed-cbe0-4d90-b693-1364dd3f7878",
   "metadata": {},
   "source": [
    "## Probabilistic Model\n",
    "Next, we define the corresponding probabilistic model. We assume that $T_a$ is known and $\\theta$ is unknown. Typically, our observations will be noisy, usually white noise. If we call our observations $y$, we can then write our measurement model as\n",
    "$$y_n = \\theta T_n + (1 - \\theta) T_a + e_n,$$\n",
    "where $e_n \\sim \\mathcal{N}(0, \\tau^{-1})$ is white noise with precision parameter $\\tau$. We assume in this example that the precision of the measurement noise is known. We could now apply a [Nonlinear Kalman Filtering](https://github.com/biaslab/ForneyLab.jl/blob/master/demo/nonlinear_kalman_filter.ipynb) from ForneyLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117d461-e8f8-4800-9aae-eb0ddbf361cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_τ = 0.04 # Observation precision = inv(sqrt(σ^2)) where σ is observation standard deviation\n",
    "generate_data(rng, n, states, τ) = states + randn(rng, n) * inv(sqrt(τ))\n",
    "\n",
    "y_ = generate_data(rng, N + 1, T_, real_τ)\n",
    "plot(time, T_, xlabel = L\"t~(\\textrm{s})\", ylabel = L\"T~(\\textrm{K})\", color = \"orange\", label = \"True\", xlim = (time[1], time[end]), ylim = (240, 320))\n",
    "scatter!(time, y_, color = \"orange\", label = \"Observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a73077-383d-47e1-bbc0-6adf9649d877",
   "metadata": {},
   "source": [
    "Now that we have the data, we can perform inference. This involves first defining the priors, and telling ForneyLab the relation between the parameters and the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb58014-c165-42d3-a6bd-914a33b9aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for prior distributions\n",
    "m_x_0 = 300 # Temperature\n",
    "v_x_0 = 400\n",
    "m_θ_0 = 2 # Parameter to identify\n",
    "v_θ_0 = 1\n",
    "# Probabilistic Model\n",
    "fg = FactorGraph()                                                          # Start model specification\n",
    "@RV θ ~ Gaussian(placeholder(:m_θ), placeholder(:v_θ), id=:θ)               # Prior for θ\n",
    "@RV x_tmin1 ~ Gaussian(placeholder(:m_x), placeholder(:v_x), id=:x_tmin1)   # Define previous state\n",
    "g(x_tmin1, θ) = θ * x_tmin1 + (1 - θ) * T_a                                 # Nonlinear state transition function\n",
    "@RV x_t ~ Delta{Unscented}(x_tmin1, θ; g=g, id=:x_t)                        # State transition node\n",
    "@RV y_t ~ Gaussian{Precision}(x_t, real_τ, id=:y_t)                         # Observation likelihood\n",
    "placeholder(y_t, :y_t);                                                     # Tell ForneyLab that variable y_t will be observed later on\n",
    "# Define sum-product message passing procedure with state x_t and fertility r as parameters of interest\n",
    "algo = messagePassingAlgorithm([x_t, θ])\n",
    "# Compile message passing procedure to an inference algorithm\n",
    "code = algorithmSourceCode(algo);\n",
    "# Import compiled functions to workspace\n",
    "eval(Meta.parse(code));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56650ee0-0ae7-4bc1-b20b-e0dc523d579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays for storing parameter estimates\n",
    "m_x_t = Vector{Float64}(undef, N + 1)\n",
    "v_x_t = Vector{Float64}(undef, N + 1)\n",
    "m_θ_t = Vector{Float64}(undef, N + 1)\n",
    "v_θ_t = Vector{Float64}(undef, N + 1)\n",
    "\n",
    "# Initialize previous parameter estimates\n",
    "m_x_tmin1 = m_x_0\n",
    "v_x_tmin1 = v_x_0\n",
    "m_θ_tmin1 = m_θ_0\n",
    "v_θ_tmin1 = v_θ_0\n",
    "\n",
    "# Recursive estimation procedure (posteriors at t => priors at t+1)\n",
    "for t ∈ 1:(N + 1)    \n",
    "    # Store data for current time-step\n",
    "    data = Dict(:y_t => y_[t],\n",
    "                :m_x => m_x_tmin1,\n",
    "                :v_x => v_x_tmin1,\n",
    "                :m_θ => m_θ_tmin1,\n",
    "                :v_θ => v_θ_tmin1)\n",
    "    # Estimate marginal distributions of interest (x_t and θ)\n",
    "    marginals = step!(data)  \n",
    "    # Extract parameters of estimated marginal distributions\n",
    "    (m_x, v_x) = ForneyLab.unsafeMeanCov(marginals[:x_t])\n",
    "    (m_θ, v_θ) = ForneyLab.unsafeMeanCov(marginals[:θ])\n",
    "    # Reset parameter estimate arrays for next time-step\n",
    "    m_x_tmin1 = m_x_t[t] = m_x\n",
    "    v_x_tmin1 = v_x_t[t] = v_x\n",
    "    m_θ_tmin1 = m_θ_t[t] = m_θ\n",
    "    v_θ_tmin1 = v_θ_t[t] = v_θ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6590f-79ee-4b6d-ac1e-f8357ec7300f",
   "metadata": {},
   "source": [
    "We can now visualise the results. The UKF simultaneously estimates the parameter and the true state. As we make more measurements, we would expect our parameter estimates to become better, which should also improve the state estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c6d0f-25b8-400c-bfee-e4ac364e398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_T = plot(time, T_, xlabel = L\"t~(\\textrm{s})\", ylabel = L\"T~(\\textrm{K})\", color = \"orange\", label = \"True\", xlim = (time[1], time[end]), ylim = (240, 320))\n",
    "plot!(p_T, time, m_x_t, color = \"blue\", ribbon = 2 * sqrt.(v_x_t), alpha = 0.25, fill_color = \"blue\", label = \"Bayesian\")\n",
    "scatter!(p_T, time, y_, color = \"orange\", label = \"Observed\")\n",
    "# savefig(p_T, \"Results\\\\Explore\\\\UKF\\\\LSSM_1_block_Forney_temp.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98ed2d-1a58-4a3f-b6a6-a9c6e389a203",
   "metadata": {},
   "source": [
    "The plot above shows the mode of the posterior in blue, with a ribbon that is two standard deviations wide on each side (so that it contains roughly 95 % of the probability mass). Around $t = 2000~\\second$, the state gets significantly overestimated, but over time this error appears to decay. \n",
    "\n",
    "We can also visualise the evolution of the parameter posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a1f2e-136b-4ea1-b134-9feba9488a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_p = plot(time, m_θ_t, color = \"blue\", ribbon = 2 * sqrt.(v_θ_t), alpha = 0.25, fill_color = \"blue\", xlabel = L\"t~(\\textrm{s})\", ylabel = L\"θ\", label = \"Bayesian\", xlim = (time[1], time[end]), ylim = (0, 3))\n",
    "hline!([real_θ], color = \"orange\", label = \"True\")\n",
    "# savefig(p_p, \"Results\\\\Explore\\\\UKF\\\\LSSM_1_block_Forney_param.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dcc25-e883-43b2-8e7b-9da0f5b91134",
   "metadata": {},
   "outputs": [],
   "source": [
    "(last(m_θ_t) - real_θ) / real_θ # Relative error of the mean of the final posterior of θ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e400a-2c33-46aa-999a-467dec931b29",
   "metadata": {},
   "source": [
    "As the plot above suggests, this approach has been very successful: the mean of the posteriors of $\\theta$ quickly goes to the true value of $\\theta$. The relative error is only about 0.5 %. \n",
    "\n",
    "We can further quantify how good our estimate is by determining the (empirical) risk. A common choice of risk is the Mean Square Error MSE, which corresponds to a quadratic loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb5a9e-58bf-414c-8fd7-96cb0b705d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "MSE(true_θ, post_mean, post_var) = (post_mean - true_θ)^2 + post_var\n",
    "\n",
    "mse_prior = MSE(real_θ, 2, 1)\n",
    "mse_post = MSE(real_θ, m_θ_t[end], v_θ_t[end])\n",
    "mse_prior, mse_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a671b94-bc26-4f79-ba8e-35f938de6d0c",
   "metadata": {},
   "source": [
    "Clearly, our risk has significantly decreased after the inference. We have to put the prior and posterior in different plots, because they have vastly different scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f4eb3-4997-478f-b617-74cfc5ea5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_MSE_prior = plot(Distributions.Normal(2, 1), ylim = (0, 0.5), xlabel = L\"θ\", ylabel = \"Density\", label = L\"Prior $θ$\", title = \"Prior\")\n",
    "vline!(p_MSE_prior, [real_θ], label = L\"True $θ$\")\n",
    "annotate!(p_MSE_prior, (0.05, 0.95), (L\"\\textrm{MSE} = %$(round(mse_prior, sigdigits = 2))\", 12, :left))\n",
    "p_MSE_post = plot(Distributions.Normal(m_θ_t[end], sqrt(v_θ_t[end])), ylim = (0, 15), xlabel = L\"θ\", ylabel = \"Density\", label = L\"Posterior $θ$\",  title = \"Posterior\")\n",
    "vline!([real_θ], label = L\"True $θ$\")\n",
    "annotate!(p_MSE_post, (0.05, 0.95), (L\"\\textrm{MSE} = %$(round(mse_post, sigdigits = 2))\", 12, :left))\n",
    "p_MSE = plot(p_MSE_prior, p_MSE_post, layout = (1, 2), size = (800, 400), leftmargin = 6mm)\n",
    "# savefig(p_MSE, \"Results\\\\Explore\\\\UKF\\\\LSSM_1_block_Forney_MSE.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd047c-fa31-4f2c-88ae-9e770d2dde87",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Unfortunately, it does not seem to be possible to use a nonlinear Kalman filter for higher dimensional versions of our problem in ForneyLab. Hence, we must look to other approximate methods, since we do not have the time to implement such a nonlinear Kalman filter. We return to the [main Julia notebook](sysid-thermal-AR.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
