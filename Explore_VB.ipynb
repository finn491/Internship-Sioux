{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7288502d",
   "metadata": {},
   "source": [
    "# Internship Finn Sherry @ Sioux Mathware\n",
    "\n",
    "---\n",
    "\n",
    "# Bayesian grey-box system identification for thermal effects: VB using BayesPy\n",
    "In this notebook, we apply the Variational Bayes from [BayesPy](https://github.com/bayespy), developed by Luttinen, to a simplified model of our to see whether VB is a viable approximation method for the rest of this project. This notebook continues on from the Variational Bayes part of my [main (Julia) notebook](sysid-thermal-AR.ipynb). \n",
    "\n",
    "Last update: 20-07-2022\n",
    "\n",
    "$\\renewcommand{\\vec}[1]{\\boldsymbol{\\mathrm{#1}}}$\n",
    "$\\newcommand{\\covec}[1]{\\hat{\\vec{#1}}}$\n",
    "$\\newcommand{\\mat}[1]{\\boldsymbol{\\mathrm{#1}}}$\n",
    "$\\newcommand{\\inv}[1]{#1^{-1}}$\n",
    "$\\newcommand{\\Expectation}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Variance}{\\mathbb{V}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77959d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayespy.nodes import GaussianARD, GaussianMarkovChain, Gamma, Dot\n",
    "from bayespy.inference import VB\n",
    "from bayespy.utils import random\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f2d653",
   "metadata": {},
   "source": [
    "## 2-D System: Adding in an extra block\n",
    "### Physical Model\n",
    "Next, we make the problem more complicated by considering a system of 2 materials with 2 temperature sensors. Ignoring input heats and assuming the influence of convection, we may write the equations govering the evolution of our system as\n",
    "$$\\mat{M} \\dot{\\vec{T}} = \\mat{K} \\vec{T},$$\n",
    "where $\\mat{K}$ contains a single unknown conduction coefficient $k$:\n",
    "$$\\mat{K} = \n",
    "\\begin{pmatrix}\n",
    "-k & k \\\\\n",
    "k & -k\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "[comment]: # (and finally $\\vec{T}_a$ is the ambient temperature at both blocks, which we will assume for the moment is in both cases constantly equal to some known $T_a$.)\n",
    "Discretising using a forward difference, we now find by rearranging the terms that\n",
    "$$\n",
    "\\vec{T}_{n + 1} = \\underbrace{(\\mat{I} + \\Delta t \\inv{\\mat{M}} \\mat{K})}_{\\mat{A}} \\vec{T}_n + \\vec{q}_n,\n",
    "$$\n",
    "where $\\vec{q}_n$ is the process noise. Finally, our measurement model is given by\n",
    "$$\n",
    "\\vec{y}_n = \\mat{I} \\vec{T}_n + \\vec{r}_n,\n",
    "$$\n",
    "where $\\vec{r}_n$ is the normally distributed measurement noise.\n",
    "\n",
    "\n",
    "\n",
    "[comment]: # (These are subject to the following dynamics:)\n",
    "\n",
    "[comment]: # ($$M\\dot{T} = K(\\theta)T + B(\\theta)u \\, ,$$)\n",
    "\n",
    "[comment]: # (where the material properties are described by:)\n",
    "\n",
    "[comment]: # ($$M = \\begin{bmatrix} m_1 c_{p, 1} & 0 & 0 \\\\\n",
    "0 & m_2 c_{p, 2} & 0 \\\\\n",
    "0 & 0 & m_3 c_{p, 3} \\end{bmatrix} \\, ,$$)\n",
    "\n",
    "[comment]: # (the conductance by:)\n",
    "\n",
    "[comment]: # ($$K(\\theta) = \\begin{bmatrix} -h_{12} -h_a(T_1) & h_{12} & 0 \\\\\n",
    "h_{12} & -h_{12}-h_{23}-h_a(T_2) & h_{23} \\\\\n",
    "0 & h_{23} & -h_{23}-h_a(T_3) \\end{bmatrix} \\, ,$$)\n",
    "\n",
    "[comment]: # (and the input by:)\n",
    "\n",
    "[comment]: # ($$B(\\theta) u = \\begin{bmatrix} \\, h_a(T_1) \\\\ h_a(T_2) \\\\ h_a(T_3) \\, \\end{bmatrix} T_a \\, ,$$)\n",
    "\n",
    "[comment]: # \"where $u=T_a$ represents the ambient temperature. We assume $M$ is matrix of (known) constants. Later on, we will add a convection term. \"\n",
    " \n",
    "[comment]: # (The parameters consist of two contact conductances $h_{12}$, $h_{23}$, and the effect of the ambient temperature $h_a$:)\n",
    "\n",
    "[comment]: # ($$\\theta = \\begin{bmatrix} \\, h_{12} & h_{23} & h_a(T_1) & h_a(T_2) & h_a(T_3) \\, \\end{bmatrix}^{\\top} \\, .$$)\n",
    "\n",
    "[comment]: # (<mark>It is unclear to me what the $h_a(T_i)$ represent. Considering that they're written as functions, I assume that there is a temperature dependence $\\theta(T)$. But for now, I am going to assume that they are static parameters. </mark>)\n",
    "\n",
    "[comment]: # (If we discretize, we get:)\n",
    "\n",
    "[comment]: # ($$\\begin{align}\n",
    "M\\dot{T} &= K(\\theta)T + B(\\theta)u \\\\\n",
    "\\dot{T} &= M^{-1} K(\\theta)T + M^{-1}B(\\theta)u \\\\\n",
    "\\frac{T_{k+1} - T_k}{\\Delta t} &= M^{-1} K(\\theta)T_k + M^{-1}B(\\theta)u_k \\\\\n",
    "T_{k+1} - T_k &= \\Delta t M^{-1} K(\\theta)T_k + \\Delta t M^{-1}B(\\theta)u_k \\\\\n",
    "T_{k+1} &= \\big(\\Delta t M^{-1} K(\\theta) + I \\big)T_k + \\Delta t M^{-1}B(\\theta)u_k \\, .\\\\\n",
    "\\end{align}$$)\n",
    "\n",
    "[comment]: # (We can view this as a vector autoregressive model with exogenous input.)\n",
    "\n",
    "[comment]: # (## Analytical Solution)\n",
    "\n",
    "[comment]: # (If we indeed assume $\\eta$ to be constant, we can in this case still get a closed form analytical description of the evolution of the posterior of $\\theta$.)\n",
    "\n",
    "[comment]: # \"In Mathematica, we define the prior as \n",
    "$$p(\\theta \\mid T_k) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(\\theta - \\mu)^2}{2 \\sigma^2}\\right),$$\n",
    "and the likelihood as\n",
    "$$p(T_{k + 1} \\mid \\theta, T_k) = \\sqrt{\\frac{\\gamma}{2 \\pi}} \\exp\\left(-\\gamma \\frac{(T_{k + 1} - \\theta T_k - \\eta T_a)^2}{2}\\right).$$\n",
    "Then we simply find the resulting posterior by computing (using Bayes' rule)\n",
    "$$p(\\theta \\mid T_{k + 1}, T_k) = \\frac{p(T_{k + 1} \\mid \\theta, T_k) p(\\theta \\mid T_k)}{\\int_{-\\infty}^{\\infty} p(T_{k + 1} \\mid \\theta, T_k) p(\\theta \\mid T_k) d \\theta}.$$\n",
    "Mathematica tells us (after applying FullSimplify) that\n",
    "$$p(\\theta \\mid T_{k + 1}, T_k) = \\sqrt{\\frac{1 + T_k^2 \\gamma \\sigma^2}{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1 + T_k^2 \\gamma \\sigma^2}{2 \\sigma^2} \\left(\\theta - \\frac{\\mu + T_k T_{k + 1} \\gamma \\sigma^2 + \\eta T_a T_k \\gamma \\sigma^2}{1 + T_k^2 \\gamma \\sigma^2}\\right)^2\\right),$$\n",
    "from which we can see that $\\theta \\mid T_{k + 1}, T_k \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$, with\n",
    "$$\\mu_1 = \\frac{\\mu + (T_{k + 1} + \\eta T_a) T_k  \\gamma \\sigma^2}{1 + T_k^2 \\gamma \\sigma^2}, \\text{ and } \\sigma_1^2 = \\frac{\\sigma^2}{1 + T_k^2 \\gamma \\sigma^2}.$$\"\n",
    "\n",
    "\n",
    "[comment]: # \"From these equations, we obtain a probabilistic model of the form:\n",
    "$$p(T_{n + 1} \\mid \\theta, T_k) = \\mathcal{N}(T_{n + 1} \\mid \\theta T_n + (1 - \\theta) T_a, \\gamma^{-1}) \\, .$$\n",
    "We should pose a prior on $\\theta$ and we apply the Autoregressive model from ReactiveMP.jl: https://biaslab.github.io/ReactiveMP.jl/stable/examples/autoregressive/. For now, let's fix $\\gamma = 1e8$ and estimate it simultaneously later on.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66e5e5",
   "metadata": {},
   "source": [
    "### Simulate Data\n",
    "We start by visualising the evolution of the temperatures in our system. Since we have no input heats or convection, this should be pretty boring. We generate an unreasonably large amount of data to give the inference the greatest possible chance of succeeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38615319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Properties\n",
    "M = 2 # Observation dimension\n",
    "D = 2 # Latent dimension\n",
    "N = 1001 # Observations\n",
    "Dt = 1. # Time step for discretisation\n",
    "times = [Dt * i for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State properties\n",
    "k = 5 # Conduction coefficient\n",
    "mcp_1 = 2e3\n",
    "mcp_2 = 1.5e3\n",
    "Mconst = np.diag([mcp_1, mcp_2])\n",
    "# a = np.linalg.inv(np.identity(D) - Dt * np.matmul(np.linalg.inv(Mconst), np.array([[-k, k], [k, -k]]))) ### Backward Euler\n",
    "a = np.identity(D) + Dt * np.matmul(np.linalg.inv(Mconst), np.array([[-k, k], [k, -k]])) # Transition matrix A\n",
    "x_1_0 = 270\n",
    "x_2_0 = 330\n",
    "std_noise_x = 0 # No process noise because unidentifiability makes that too messy for the moment\n",
    "std_noise_y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = np.empty((N, D))\n",
    "y = np.empty((N, M))\n",
    "x[0] = np.array([x_1_0, x_2_0])\n",
    "y[0] = x[0] + std_noise_y * np.random.randn(M)\n",
    "for n in range(N - 1):\n",
    "    x[n + 1] = np.dot(a, x[n]) + std_noise_x * np.random.randn(D)\n",
    "    y[n + 1] = x[n + 1] + std_noise_y * np.random.randn(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (8, 8));\n",
    "ax.grid(True);\n",
    "ax.set_prop_cycle('color', [\"b\", \"r\"]);\n",
    "ax.plot(times, x, label = [r\"$T_1$\", r\"$T_2$\"]);\n",
    "ax.scatter(times, y[:, 0], label = r\"$y_1$\");\n",
    "ax.scatter(times, y[:, 1], label = r\"$y_2$\");\n",
    "ax.set_xlim(times[0], ceil(times[-1]));\n",
    "ax.set_ylim(260, 340);\n",
    "ax.set_xlabel(r\"$t~(\\mathrm{s})$\");\n",
    "ax.set_ylabel(r\"$T~(\\mathrm{K})$\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044bc95",
   "metadata": {},
   "source": [
    "### Probabilistic Model\n",
    "We can now define the priors of our system. $\\alpha$ and $\\nu$ are so-called Automatic Relevance Determination (ARD) parameters, which should automatically make less relevant elements in our matrices play a smaller role in the inference. Note that Gamma is shape-rate: if we define some random variable $x \\sim \\Gamma(a, b)$, then \n",
    "$$\\Expectation[x] = \\frac{a}{b}, \\text{ and } \\Variance(x) = \\frac{a}{b^2}.$$\n",
    "Hence, for an uninformative prior, we should make $b$ small.\n",
    "\n",
    "$\\mat{A}$is our state transition matrix. We define a prior per column, with corresponding ARD parameter. This is not really a sensible thing to do, however: there is no reason to believe that elements within a column are necessarily roughly equally relevant, while those in different columns could still be equally relevant. It would be more reasonable to have a parameter for each entry. Moreover, while entries in a column can be dependent, those in different columns are treated as being independent, which clearly is not true. Regardless, for the time being we work with this setup, since it somewhat works. If the results are promising, we can come back and make improvements.\n",
    "\n",
    "We start by defining the probabilistic model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(M, N, D, nu_a, nu_b):\n",
    "    # Transition\n",
    "    alpha = Gamma(1e-5, 1e-5, plates = (D,), name = \"alpha\") # ARD\n",
    "    A = GaussianARD(0, alpha, shape = (D,), plates = (D,), name = \"A\")\n",
    "    A.initialize_from_value(np.identity(D))\n",
    "    # Process\n",
    "    nu = Gamma(nu_a, nu_b, plates = (D,), name = \"nu\") # Innovation: not really sure what this does...\n",
    "    X = GaussianMarkovChain(300 * np.ones(D), np.identity(D) / 900, A, nu, n = N, name = \"X\")\n",
    "    X.initialize_from_value(np.random.randn(N, D))\n",
    "    # Observation\n",
    "    tau = Gamma(1e0, 1e0, name = \"tau\")\n",
    "    tau.initialize_from_value(1e0)\n",
    "    Y = GaussianARD(X, tau, name = \"Y\")\n",
    "    # Variational Inference\n",
    "    Q = VB(X, A, alpha, nu, tau, Y)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce29318",
   "metadata": {},
   "source": [
    "... and subsequently perform VB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b923c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(y, M, N, D, nu_a, nu_b):\n",
    "    Q = model(M, N, D, nu_a, nu_b)\n",
    "    Q['Y'].observe(y)\n",
    "    Q.update()\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Q = infer(y, M, N, D, 50, 1)\n",
    "inferred_X = my_Q['X'].get_moments()[0]\n",
    "inferred_A = my_Q['A'].get_moments()[0]\n",
    "inferred_tau = my_Q['tau'].get_moments()[0]\n",
    "inferred_nu = my_Q['nu'].get_moments()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99c5df-2cd9-4519-be46-44ce9672a39f",
   "metadata": {},
   "source": [
    "It seems like only a single iteration was necessary to converge. In less than a second, we were able to perform batch estimation using a dataset of about 2000 observations. I could imagine that then a single recursive update step would take mere milliseconds; VB indeed seems quite suitable for online estimation. However, we should also see whether the state and parameter estimates are any good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (8, 8));\n",
    "ax.grid(True);\n",
    "ax.set_prop_cycle('color', [\"b\", \"r\"])\n",
    "ax.plot(times, x, label = [r\"$T_1$\", r\"$T_2$\"]);\n",
    "ax.scatter(times, y[:, 0], label = r\"$y_1$\");\n",
    "ax.scatter(times, y[:, 1], label = r\"$y_2$\");\n",
    "ax.set_prop_cycle('color', [\"g\", \"orange\"])\n",
    "ax.plot(times, inferred_X, label = [r\"$T_1$ Bayes\", r\"$T_2$ Bayes\"])\n",
    "ax.set_xlim(times[0], round(times[-1]));\n",
    "ax.set_ylim(260, 340);\n",
    "ax.set_xlabel(r\"$t~(\\mathrm{s})$\");\n",
    "ax.set_ylabel(r\"$T~(\\mathrm{K})$\");\n",
    "ax.legend();\n",
    "# fig.savefig(\"Results/Explore/VB/2_blocks.pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3be9b0",
   "metadata": {},
   "source": [
    "One thing to note is that the Bayesian MAP estimate for the state is somewhat jagged. Since we have no process noise, it would be desirable for the estimate to be smooth. I am not sure what the innovation $\\nu$ does: it seems like it is the precision of the process noise. If we make it too small, the MAP estimate goes through all the observations, seemingly ignoring the underlying dynamics; if we make it too large, the MAP estimate seems to completely ignore all observations. In general, the estimate seems to be very sensitive to the prior of $\\nu$.\n",
    "\n",
    "Next, we extract information about our unknown quantity $k$. Here we run into a disadvantage of using VB in this way: we never defined $k$ as a quantity in our probabilistic model, only $\\mat{A}$. Hence, we somehow have to use our posterior for $\\mat{A}$ and the dependence of $\\mat{A}$ on $k$ to estimate $k$. For a point estimate, I have the following straightforward idea: simply compute $k$ from each component of the MAP estimate $\\mat{A}$, and take the average.\n",
    "\n",
    "Recall that\n",
    "$$\\mat{A} = \\mat{I} + \\Delta t \\inv{\\mat{M}} \\mat{K}.$$\n",
    "Consequently, we can conclude that\n",
    "$$\\mat{K} = \\frac{1}{\\Delta t} \\mat{M} (\\mat{A} - \\mat{I}).$$\n",
    "Then, our point estimate for $k$ will be \n",
    "$$\\hat{k} = \\frac{-K_{1, 1} + K_{1, 2} + K_{2, 1} - K_{2, 2}}{4}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ec5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_K = np.matmul(Mconst, inferred_A - np.identity(2)) / Dt\n",
    "inferred_k = (-inferred_K[0, 0] + inferred_K[0, 1] + inferred_K[1, 0] - inferred_K[1, 1]) / 4\n",
    "k, inferred_k, (inferred_k - k) / k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d9a96",
   "metadata": {},
   "source": [
    "This point estimate is pretty good, but the influence of $\\nu$ is very big. Vhanging its prior (say to `Gamma(1, 1)`) drastically changes the quality of the estimate (with this change the estimate is 60 % more than the true value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac1256",
   "metadata": {},
   "source": [
    "## 3-D System: Another one\n",
    "### Physical Model\n",
    "We now augment the problem by adding in an extra block. The dynamics of the system are still governed by\n",
    "$$ \\mat{M} \\dot{\\vec{T}} = \\mat{K} \\vec{T},$$\n",
    "but now $\\mat{K}$ contains two unknown conduction coefficient $k_{12}$ and $k_{23}$:\n",
    "$$\\mat{K} = \n",
    "\\begin{pmatrix}\n",
    "-k_{12} & k_{12} & 0 \\\\\n",
    "k_{12} & -(k_{12} + k_{23}) & k_{23} \\\\\n",
    "0 & k_{23} & -k_{23}\n",
    "\\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e1bcbd",
   "metadata": {},
   "source": [
    "### Simulate Data\n",
    "Let's again start by visualising the evolution of the temperatures in our system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaee0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Properties\n",
    "M = 3 # Observation dimension\n",
    "D = 3 # Latent dimension\n",
    "N = 1001 # Observations\n",
    "Dt = 1. # Time step for discretisation\n",
    "times = [Dt * i for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State properties\n",
    "k12 = 5 # Conduction coefficient\n",
    "k23 = 4\n",
    "mcp_1 = 2e3\n",
    "mcp_2 = 1.5e3\n",
    "mcp_3 = 2.5e3\n",
    "Mconst = np.diag([mcp_1, mcp_2, mcp_3])\n",
    "# a = np.linalg.inv(np.identity(D) - Dt * np.matmul(np.linalg.inv(Mconst), np.array([[-k12, k12, 0], [k12, -(k12 + k23), k23], [0, k23, -k23]]))) # Transition matrix A\n",
    "a = np.identity(D) + Dt * np.matmul(np.linalg.inv(Mconst), np.array([[-k12, k12, 0], [k12, -(k12 + k23), k23], [0, k23, -k23]]))\n",
    "x_1_0 = 270\n",
    "x_2_0 = 330\n",
    "x_3_0 = 320\n",
    "std_noise_x = 0 # No process noise because unidentifiability makes that too messy for the moment\n",
    "std_noise_y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c528cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = np.empty((N, D))\n",
    "y = np.empty((N, M))\n",
    "x[0] = np.array([x_1_0, x_2_0, x_3_0])\n",
    "y[0] = x[0] + std_noise_y * np.random.randn(M)\n",
    "for n in range(N - 1):\n",
    "    x[n + 1] = np.dot(a, x[n]) + std_noise_x * np.random.randn(D)\n",
    "    y[n + 1] = x[n + 1] + std_noise_y * np.random.randn(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (8, 8));\n",
    "ax.grid(True);\n",
    "ax.set_prop_cycle('color', [\"b\", \"r\", \"purple\"]);\n",
    "ax.plot(times, x, label = [r\"$T_1$\", r\"$T_2$\", r\"$T_3$\"]);\n",
    "ax.scatter(times, y[:, 0], label = r\"$y_1$\");\n",
    "ax.scatter(times, y[:, 1], label = r\"$y_2$\");\n",
    "ax.scatter(times, y[:, 2], label = r\"$y_3$\");\n",
    "ax.set_xlim(times[0], round(times[-1]));\n",
    "ax.set_ylim(260, 340);\n",
    "ax.set_xlabel(r\"$t~(\\mathrm{s})$\");\n",
    "ax.set_ylabel(r\"$T~(\\mathrm{K})$\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce993bb",
   "metadata": {},
   "source": [
    "### Probabilistic Model\n",
    "Because of the way we set up the probabilistic model for the case with 2 blocks, we can easily reuse the existing functions. Hence, we can immediately perform parameter identification. First, we pose the prior $\\Gamma(100, 1)$ on $\\nu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5479c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Q_medium_precision = infer(y, M, N, D, 100, 1)\n",
    "inferred_X_medium_precision = my_Q_medium_precision['X'].get_moments()[0]\n",
    "inferred_A_medium_precision = my_Q_medium_precision['A'].get_moments()[0]\n",
    "inferred_tau_medium_precision = my_Q_medium_precision['tau'].get_moments()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2fe1f-c45a-4669-8523-6fef3ae2edf5",
   "metadata": {},
   "source": [
    "Adding the extra dimensions does not seem to have slowed down inference. This is a promising sign: future model expansions might similarly influence computation time in a negligible manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (8, 8));\n",
    "ax.grid(True);\n",
    "ax.set_prop_cycle('color', [\"b\", \"r\", \"purple\"]);\n",
    "ax.plot(times, x, label = [r\"$T_1$\", r\"$T_2$\", r\"$T_3$\"]);\n",
    "ax.scatter(times, y[:, 0], label = r\"$y_1$\");\n",
    "ax.scatter(times, y[:, 1], label = r\"$y_2$\");\n",
    "ax.scatter(times, y[:, 2], label = r\"$y_3$\");\n",
    "ax.set_prop_cycle('color', [\"g\", \"orange\", \"y\"]);\n",
    "ax.plot(times, inferred_X_medium_precision, label = [r\"$T_1$ Bayes\", r\"$T_2$ Bayes\", r\"$T_3$ Bayes\"])\n",
    "ax.set_xlim(times[0], round(times[-1]));\n",
    "ax.set_ylim(260, 340);\n",
    "ax.set_xlabel(r\"$t~(\\mathrm{s})$\");\n",
    "ax.set_ylabel(r\"$T~(\\mathrm{K})$\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eca50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_A, a, (inferred_A_medium_precision - a) / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_K = np.matmul(Mconst, inferred_A_medium_precision - np.identity(3)) / Dt\n",
    "inferred_k12 = (-inferred_K[0, 0] + inferred_K[1, 0] + inferred_K[0, 1]) / 3\n",
    "inferred_k23 = (-inferred_K[2, 2] + inferred_K[2, 1] + inferred_K[1, 2]) / 3\n",
    "k12, inferred_k12, (inferred_k12 - k12) / k12, k23, inferred_k23, (inferred_k23 - k23) / k23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df31c3",
   "metadata": {},
   "source": [
    "Now, the estimates are not very good. They are again very sensitive to the prior of $\\nu$. Maybe part of the problem is the ARD. We also consider other priors for $\\nu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f445f-ab9d-4178-ba05-82ff4fcc3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Q_low_precision = infer(y, M, N, D, 1, 1)\n",
    "inferred_X_low_precision = my_Q_low_precision['X'].get_moments()[0]\n",
    "inferred_A_low_precision = my_Q_low_precision['A'].get_moments()[0]\n",
    "inferred_tau_low_precision = my_Q_low_precision['tau'].get_moments()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75841ffd-80b5-4341-820e-5ee104849e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Q_high_precision = infer(y, M, N, D, 10000, 1)\n",
    "inferred_X_high_precision = my_Q_high_precision['X'].get_moments()[0]\n",
    "inferred_A_high_precision = my_Q_high_precision['A'].get_moments()[0]\n",
    "inferred_tau_high_precision = my_Q_high_precision['tau'].get_moments()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4d312-e27a-4b82-aa4c-b3babade33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize = (24, 8));\n",
    "ax[0].grid(True);\n",
    "ax[0].set_prop_cycle('color', [\"b\", \"r\", \"purple\"]);\n",
    "ax[0].plot(times, x, label = [r\"$T_1$\", r\"$T_2$\", r\"$T_3$\"]);\n",
    "ax[0].scatter(times, y[:, 0], label = r\"$y_1$\");\n",
    "ax[0].scatter(times, y[:, 1], label = r\"$y_2$\");\n",
    "ax[0].scatter(times, y[:, 2], label = r\"$y_3$\");\n",
    "ax[0].set_prop_cycle('color', [\"g\", \"orange\", \"y\"]);\n",
    "ax[0].plot(times, inferred_X_low_precision, label = [r\"$T_1$ Bayes\", r\"$T_2$ Bayes\", r\"$T_3$ Bayes\"])\n",
    "ax[0].set_xlim(times[0], round(times[-1]));\n",
    "ax[0].set_ylim(260, 340);\n",
    "ax[0].set_xlabel(r\"$t~(\\mathrm{s})$\");\n",
    "ax[0].set_ylabel(r\"$T~(\\mathrm{K})$\");\n",
    "ax[0].set_title(r\"$\\nu \\sim \\mathrm{Gamma}(1, 1)$\");\n",
    "ax[0].legend();\n",
    "ax[1].grid(True);\n",
    "ax[1].set_prop_cycle('color', [\"b\", \"r\", \"purple\"]);\n",
    "ax[1].plot(times, x, label = [r\"$T_1$\", r\"$T_2$\", r\"$T_3$\"]);\n",
    "ax[1].scatter(times, y[:, 0], label = r\"$y_1$\");\n",
    "ax[1].scatter(times, y[:, 1], label = r\"$y_2$\");\n",
    "ax[1].scatter(times, y[:, 2], label = r\"$y_3$\");\n",
    "ax[1].set_prop_cycle('color', [\"g\", \"orange\", \"y\"]);\n",
    "ax[1].plot(times, inferred_X_medium_precision, label = [r\"$T_1$ Bayes\", r\"$T_2$ Bayes\", r\"$T_3$ Bayes\"])\n",
    "ax[1].set_xlim(times[0], round(times[-1]));\n",
    "ax[1].set_ylim(260, 340);\n",
    "ax[1].set_title(r\"$\\nu \\sim \\mathrm{Gamma}(100, 1)$\");\n",
    "ax[1].set_xlabel(r\"$t~(\\mathrm{s})$\");\n",
    "ax[1].set_ylabel(r\"$T~(\\mathrm{K})$\");\n",
    "ax[1].legend();\n",
    "ax[2].grid(True);\n",
    "ax[2].set_prop_cycle('color', [\"b\", \"r\", \"purple\"]);\n",
    "ax[2].plot(times, x, label = [r\"$T_1$\", r\"$T_2$\", r\"$T_3$\"]);\n",
    "ax[2].scatter(times, y[:, 0], label = r\"$y_1$\");\n",
    "ax[2].scatter(times, y[:, 1], label = r\"$y_2$\");\n",
    "ax[2].scatter(times, y[:, 2], label = r\"$y_3$\");\n",
    "ax[2].set_prop_cycle('color', [\"g\", \"orange\", \"y\"]);\n",
    "ax[2].plot(times, inferred_X_high_precision, label = [r\"$T_1$ Bayes\", r\"$T_2$ Bayes\", r\"$T_3$ Bayes\"])\n",
    "ax[2].set_xlim(times[0], round(times[-1]));\n",
    "ax[2].set_ylim(260, 340);\n",
    "ax[2].set_title(r\"$\\nu \\sim \\mathrm{Gamma}(10^4, 1)$\");\n",
    "ax[2].set_xlabel(r\"$t~(\\mathrm{s})$\");\n",
    "ax[2].set_ylabel(r\"$T~(\\mathrm{K})$\");\n",
    "ax[2].legend();\n",
    "# fig.savefig(\"Results/Explore/VB/3_blocks_compare_priors.pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589735e-0589-440b-8714-86e1e03297b8",
   "metadata": {},
   "source": [
    "The other priors do not appear to perform much better. When we use $\\Gamma(1, 1)$, the state estimates are very jagged, which suggests that the grey-box model is being ignored compared to data. Conversely, the state estimates resulting from the $\\Gamma(10^4, 1)$ prior are nice and smooth, but seem to ignore the measurement data: especially for small $t$ the estimates are way off.\n",
    "\n",
    "We will finally compare the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7c212-34fd-49e2-b42d-b49d7fa9cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_K = np.matmul(Mconst, inferred_A_low_precision - np.identity(3)) / Dt\n",
    "inferred_k12 = (-inferred_K[0, 0] + inferred_K[1, 0] + inferred_K[0, 1]) / 3\n",
    "inferred_k23 = (-inferred_K[2, 2] + inferred_K[2, 1] + inferred_K[1, 2]) / 3\n",
    "k12, inferred_k12, (inferred_k12 - k12) / k12, k23, inferred_k23, (inferred_k23 - k23) / k23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02186db5-803e-47e3-900f-e28bbe93cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_K = np.matmul(Mconst, inferred_A_medium_precision - np.identity(3)) / Dt\n",
    "inferred_k12 = (-inferred_K[0, 0] + inferred_K[1, 0] + inferred_K[0, 1]) / 3\n",
    "inferred_k23 = (-inferred_K[2, 2] + inferred_K[2, 1] + inferred_K[1, 2]) / 3\n",
    "k12, inferred_k12, (inferred_k12 - k12) / k12, k23, inferred_k23, (inferred_k23 - k23) / k23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5aab75-4633-44d9-acd6-280e7058d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_K = np.matmul(Mconst, inferred_A_high_precision - np.identity(3)) / Dt\n",
    "inferred_k12 = (-inferred_K[0, 0] + inferred_K[1, 0] + inferred_K[0, 1]) / 3\n",
    "inferred_k23 = (-inferred_K[2, 2] + inferred_K[2, 1] + inferred_K[1, 2]) / 3\n",
    "k12, inferred_k12, (inferred_k12 - k12) / k12, k23, inferred_k23, (inferred_k23 - k23) / k23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9431628-5914-4550-badc-6b159b0860f7",
   "metadata": {},
   "source": [
    "Of the options shown here (and indeed the many other combinations of prior hyperparameters that we tried), $\\Gamma(100, 1)$ performs the best. However, the parameter estimates are still pretty bad. Notably, we see that the quality of the inference for the two conduction parameters tends to differ: when a prior gives a good estimate for $k_{12}$, it often gives a bad estimate for $k_{23}$. There does not appear to be an optimal prior that gives acceptable parameter estimates for both conduction coefficients simultaneously. Moreover, we improved the inference by playing around with the prior. There is no reason to believe that the values we found will work well in general, and we have no way to derive good hyperparameters from theory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d352dfc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "There are numerous issues with VB that make it unsuitable for application in this project, which could be mostly solved by investing a sufficient amount of time:\n",
    "- Even without real process noise, the inference is very sensitive to the prior of the innovation $\\nu$;\n",
    "- The ARD as is currently applied in BayesPy does not make a lot of sense for our problem. We would have to implement a componentwise (instead of columnwise) ARD. That might help with the sensitivity to the prior of the innovation too;\n",
    "- Our problem is somewhat more complex, involving also convection and input heats. It is not clear how these can be added using existing software;\n",
    "- It is not clear how to convert knowledge of the distribution of components in matrices into posteriors for the underlying parameters.\n",
    "\n",
    "Consequently, we will look for another approximate Bayesian inference method. We continue in the main [Julia Jupyter notebook](sysid-thermal-AR.ipynb). _try to link to correct section_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
